{
  "hash": "cea22992326e2dcd160bb09c7313c9c1",
  "result": {
    "markdown": "---\ntitle: Chi-Square and Random Forest Regression with Data Science Jobs Dataset\nauthor: Kevin B. Miller\ndate: 2024/05/07\nformat: html\ntoc: true\ntoc-location: body\ntoc-depth: 4\ncsl: assets/apa.csl\nbibliography: assets/references.bib\nimage: fig-chi-heatmap-output-1.png\ncategories:\n  - Python\n  - Descriptive Analysis\n  - Random Forest\ndescription: Do entry-level employees stand a chance in data science? Stay tuned to find out.\ngithub_url: 'https://github.com/kmiller999/capstone_report'\nmedium: Python\n---\n\nThis work represents a finalized Capstone Report for my Master's of Science in Data Analytics from Western Governors University (WGU). To comply with WGU's academic integrity guidelines, the present document was not submitted as part of my Capstone. Rather, this report represents an extension of the project that showcases my work without making my submitted materials publicly available.\n\n\n{{< pagebreak >}}\n\n\n\n## Abstract\n\nThe growth of data science as a field has produced simultaneous increases in role diversification. The present study further characterized differences between job roles by examining whether experience levels varied between data science job categories. This study focused on a subset of publicly available data comprising data science employees working full-time in the U.S. (*N* = 12,389). A Chi-Square Test of Independence with subsequent post hoc tests analyzed the association between experience level and job category. A Random Forest Regression model was used to predict salaries from experience level, job category, and work setting. Experience level composition varied between job categories (*X^2^* = 1003.54, *p* \\< .001), with each job category differing from all others (*p*-range: *p* \\< .001 - *p* = .001). Residual analysis revealed two experience level-job category intersections with substantially greater observed counts than expected: executive employees in leadership roles and entry-level employees in data analysis roles. Employees in these experience levels were otherwise underrepresented, whereas senior employees comprised a sizable portion of each job category. The Random Forest Regression model explained roughly 25% of the variance in employee salary. Salary estimates were higher for employees in machine learning or AI-related roles or senior positions, and lower for employees in data analysis roles. This study further characterized differences between data science roles, most notably for experience levels typically found in each. These findings have implications for the accessibility of these roles, particularly for entry-level employees, who may have fewer attainable job roles.\n\n\n{{< pagebreak >}}\n\n\n\n## 1: Introduction and Research Question\n\nAfter being heralded as the \"sexiest job of the 21st century\" [@davenport2012], the data scientist role met these lofty expectations [@davenport2022], and is expected to continue growing through 2032 [@datasci2024]. The responsibilities of this role grew simultaneously, requiring more specialized data science roles be created to focus on a portion of the data science process [@davenport2022]. Examples of these expanded roles include the data engineer and data analyst [@rosidi2023], with the former focusing on the quality and accessibility of data and the latter focusing on analysis and reporting of data. Yet with seemingly more job roles, demand, and resources to learn data science than ever [@davenport2022], many of those attempting to break into the field find themselves frustrated and disillusioned [@selvaraj2022].\n\nThis mismatch between the expectations and experiences of data science hopefuls underscores the need to characterize the skills and experiences necessary to succeed in a data science role. Greater transparency surrounding these factors would demystify the field as a whole and provide a better road map for employment-ready data science professionals. Using an open dataset on data science jobs from Kaggle [@zangari], this study sought to discern whether there was a categorical association between experience level and job category in data science jobs. Following a statistically significant Chi-Square Test of Independence omnibus test, the experience level composition of individual job categories would be compared using the same Chi-Square test. An additional Random Forest Regression (RFR) model was used to estimate salary from experience level, job category, and work setting.\n\n### 1.1: Research Question and Hypotheses\n\nDoes experience level composition vary between data science job categories?\n\nNull Hypothesis (H~0~): The proportion of employees at each experience level does not vary between job categories in the dataset.\n\nAlternative Hypothesis (H~A~): The proportion of employees at each experience level varies between job categories in the dataset.\n\n## 2: Data Collection\n\nThis data used in this study is publicly available on Kaggle [@zangari]. The data was originally collected and compiled by @getafu2024, and features responses from data science professionals and employers. The main adaptation made in the Kaggle version was the addition of the `job_categories` feature, which collapsed 149 unique job titles into ten general categories. This was highly advantageous for the scope of this study, as it would be difficult to compare and interpret differences between 149 different job titles.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# import useful libraries\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import Markdown, display\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# read in csv file as dataframe\njob_df = pd.read_csv('../data/jobs_in_data_2024.csv')\n```\n:::\n\n\n### 2.1: Exploratory Data Analysis\n\nThe dataset contained 14,199 records without any missing values for any columns. The chronological range of responses began in 2020, with some responses as recent as this year (i.e., 2024). Since some features pertained to similar constructs (e.g., `job_title` and `job_category`), it was important to gauge the usability of features and levels therein. For instance, some features had a large number of levels (e.g., `job_title`, `employee_residence`, and `company_location`), which can be difficult to analyze. Additionally, features like `employment_type`, `employee_residence`, and `company_location` exhibited considerable categorical imbalances, with the vast majority of responses belonging to one category.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# get initial info for df\njob_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 14199 entries, 0 to 14198\nData columns (total 12 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   work_year           14199 non-null  int64 \n 1   experience_level    14199 non-null  object\n 2   employment_type     14199 non-null  object\n 3   job_title           14199 non-null  object\n 4   salary              14199 non-null  int64 \n 5   salary_currency     14199 non-null  object\n 6   salary_in_usd       14199 non-null  int64 \n 7   employee_residence  14199 non-null  object\n 8   work_setting        14199 non-null  object\n 9   company_location    14199 non-null  object\n 10  company_size        14199 non-null  object\n 11  job_category        14199 non-null  object\ndtypes: int64(3), object(9)\nmemory usage: 1.3+ MB\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# get number of unique values for each variable \nfor column in job_df.columns:\n    print(column, len(job_df[column].value_counts()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nwork_year 5\nexperience_level 4\nemployment_type 4\njob_title 149\nsalary 2229\nsalary_currency 12\nsalary_in_usd 2578\nemployee_residence 86\nwork_setting 3\ncompany_location 74\ncompany_size 3\njob_category 10\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# get value counts for each object variable\nfor column in job_df.columns:\n    if job_df[column].dtype == object:\n        print(job_df[column].value_counts(), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nexperience_level\nSenior         9381\nMid-level      3339\nEntry-level    1063\nExecutive       416\nName: count, dtype: int64 \n\nemployment_type\nFull-time    14139\nContract        26\nPart-time       22\nFreelance       12\nName: count, dtype: int64 \n\njob_title\nData Engineer                    3059\nData Scientist                   2910\nData Analyst                     2120\nMachine Learning Engineer        1488\nResearch Scientist                454\n                                 ... \nData Analytics Associate            1\nAnalytics Engineering Manager       1\nSales Data Analyst                  1\nAWS Data Architect                  1\nConsultant Data Engineer            1\nName: count, Length: 149, dtype: int64 \n\nsalary_currency\nUSD    13146\nGBP      538\nEUR      422\nCAD       51\nAUD       12\nPLN        7\nCHF        6\nSGD        6\nBRL        4\nTRY        3\nDKK        3\nNZD        1\nName: count, dtype: int64 \n\nemployee_residence\nUnited States     12418\nUnited Kingdom      616\nCanada              371\nSpain               131\nGermany              90\n                  ...  \nAndorra               1\nUganda                1\nOman                  1\nQatar                 1\nLuxembourg            1\nName: count, Length: 86, dtype: int64 \n\nwork_setting\nIn-person    9413\nRemote       4573\nHybrid        213\nName: count, dtype: int64 \n\ncompany_location\nUnited States           12465\nUnited Kingdom            623\nCanada                    373\nSpain                     127\nGermany                    96\n                        ...  \nAndorra                     1\nQatar                       1\nMauritius                   1\nGibraltar                   1\nMoldova, Republic of        1\nName: count, Length: 74, dtype: int64 \n\ncompany_size\nM    13112\nL      919\nS      168\nName: count, dtype: int64 \n\njob_category\nData Science and Research         4675\nData Engineering                  3157\nData Analysis                     2204\nMachine Learning and AI           2148\nLeadership and Management          791\nBI and Visualization               600\nData Architecture and Modeling     419\nData Management and Strategy       115\nData Quality and Operations         79\nCloud and Database                  11\nName: count, dtype: int64 \n\n```\n:::\n:::\n\n\n::: {.cell layout-ncol='2' execution_count=6}\n``` {.python .cell-code}\n# show distribution for salary_in_usd\nsns.histplot(job_df['salary_in_usd'])\nplt.show()\n\n# show distribution for salary\nsns.histplot(job_df['salary'])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](capstone_report_files/figure-html/cell-7-output-1.png){width=593 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](capstone_report_files/figure-html/cell-7-output-2.png){width=593 height=429}\n:::\n:::\n\n\n## 3: Data Extraction and Preparation\n\nDue to categorical imbalances shown in the previous section, the dataset was filtered to only include employees residing in the U.S. (`employee_residence`), working for a U.S.-based company (`company_location`), and working full-time (`employment_type`). While this reduced the potential generalizability of findings, it was important to recognize that the data was already biased towards these groups, and any attempt to generalize beyond that would be undermined to begin with. Filtering for these categories reduced the number of observations to 12,389, which constituted less than 13% data loss. Features `work_setting` and `company_size` were not filtered to remedy categorical imbalances, as each feature contained only three levels, and the degree of the imbalance was less concerning. Potential outliers and skewness in `salary_in_usd` were not treated, as Random Forest models are robust to outliers and extreme values [@deori2023], and high salaries were potentially noteworthy in the RFR model.\n\nOf the twelve original features, only four were kept for analyses: `experience_level`, `salary_in_usd`, `work_setting`, and `job_category`. After filtering for `employment_type`, `employee_residence`, `company_location`, these three features were excluded on the basis that each was no longer a variable (i.e., each only had one value). The feature `job_category` was used place of `job_title`, as the former had far lower cardinality (i.e., ten unique value vs. 149), making it far easier to incorporate in analyses. Filtering by country meant `salary_in_usd` could be used in place of `salary` and `salary_currency`, as conversions would not be necessary. Features `work_year` and `company_size` would have been nice to add to the salary estimation Random Forest Regression model, but both had sub-optimal distributions and level structures to analyze.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# create separate df for just US employees working for US companies\nus_job_df = job_df.loc[(job_df['employee_residence'] == 'United States') & \n                       (job_df['company_location'] == 'United States')]\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# filter df to only include full-time employees\nus_job_df = us_job_df.loc[us_job_df['employment_type'] == 'Full-time']\nprint(us_job_df['employment_type'].value_counts())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nemployment_type\nFull-time    12389\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# reset index to align with observations\nus_job_df.reset_index(inplace=True, drop=True)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# keep only relevant variables\nrefined_us_job_df = us_job_df[['experience_level', 'salary_in_usd', 'work_setting',\n                               'job_category']]\n```\n:::\n\n\n### 3.A: Chi-Square Model Preparation\n\nInitial cross tabulations characterized the expected frequencies of `experience_level` and `job_category` before running the Chi-Square model (see @tbl-exp-cts). The a-priori criteria defined in this project's proposal was that any job categories with fewer than five expected counts in any cell would be combined with other job categories meeting this criteria. Such low expected counts would be abnormally low for a dataset of this size, and collapsing categories can serve as a sound solution without incurring data loss [@bewick2003]. The job categories Cloud and Database, Data Management and Strategy, and Data Quality and Operations all had fewer than five expected counts for at least one experience level, leading these to be combined into one category (see @tbl-exp-cts-1). Rather than labeling this category as \"Other,\" these categories all pertained to Data Management, which served as a more descriptive label (see @tbl-exp-cts-2). The final cross tabulation to be used in the Chi-Square analysis was saved as `trans_refined_ct`.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# create crosstab for experience_level and job_category, with margins\nimport statsmodels.api as sm\n#pd.set_option('display.float_format', lambda x: '{0:.2f}' % x)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.precision', 3)\n\n#pd.set_option('display.precision', 2)\ncross_tab = pd.crosstab(refined_us_job_df['experience_level'], \n                        refined_us_job_df['job_category'], \n                        margins=True)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# obtain expected values for crosstab\nexpected_table = sm.stats.Table(cross_tab)\n#expected_table.fittedvalues.round(2).T\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# create copy before transforming levels\nct_us_job_df = refined_us_job_df.copy(deep=True)\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# convert 3 levels to Data Management\nct_us_job_df.replace({'job_category': \n                      {'Cloud and Database': 'Data Management', \n                       'Data Management and Strategy': 'Data Management', \n                       'Data Quality and Operations': 'Data Management'}}, \n                      inplace=True)\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# define and output new contingency table\nrefined_ct = pd.crosstab(ct_us_job_df['experience_level'], \n                         ct_us_job_df['job_category'])\nrefined_expected_table = sm.stats.Table(refined_ct)\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# create expected table with margins\nmarginal_ct = pd.crosstab(ct_us_job_df['experience_level'], \n                          ct_us_job_df['job_category'], margins=True)\nmarginal_expected_table = sm.stats.Table(marginal_ct)\n```\n:::\n\n\n::: {#tbl-exp-cts .cell tbl-subcap='[\"Original (ten job categories)\",\"Refined (eight job categories)\"]' tbl-cap='Original and refined expected counts contingency tables.' layout-nrow='2' execution_count=17}\n``` {.python .cell-code}\n# create rounded, transposed versions of tables\nct_exp_md = expected_table.fittedvalues.round(2).T\nct_ref_exp_md = marginal_expected_table.fittedvalues.round(2).T\n\n# display markdown versions of tables\ndisplay(Markdown(ct_exp_md.to_markdown()))\ndisplay(Markdown(ct_ref_exp_md.to_markdown()))\n```\n\n::: {.cell-output .cell-output-display}\n| job_category                   |   Entry-level |   Executive |   Mid-level |   Senior |      All |\n|:-------------------------------|--------------:|------------:|------------:|---------:|---------:|\n| BI and Visualization           |         33.39 |       15.52 |      115.16 |   365.96 |   529.97 |\n| Cloud and Database             |          0.72 |        0.34 |        2.5  |     7.94 |    11.5  |\n| Data Analysis                  |        119.19 |       55.39 |      411.09 |  1306.42 |  1891.9  |\n| Data Architecture and Modeling |         23.77 |       11.05 |       81.97 |   260.49 |   377.23 |\n| Data Engineering               |        175.44 |       81.54 |      605.12 |  1923.03 |  2784.86 |\n| Data Management and Strategy   |          6.13 |        2.85 |       21.13 |    67.15 |    97.25 |\n| Data Quality and Operations    |          4.17 |        1.94 |       14.39 |    45.75 |    66.25 |\n| Data Science and Research      |        260.55 |      121.1  |      898.66 |  2855.9  |  4135.79 |\n| Leadership and Management      |         43.15 |       20.06 |      148.84 |   472.99 |   684.97 |\n| Machine Learning and AI        |        114.02 |       52.99 |      393.27 |  1249.8  |  1809.91 |\n| All                            |        780.46 |      362.73 |     2691.86 |  8554.57 | 12388.4  |\n:::\n\n::: {.cell-output .cell-output-display}\n| job_category                   |   Entry-level |   Executive |   Mid-level |   Senior |      All |\n|:-------------------------------|--------------:|------------:|------------:|---------:|---------:|\n| BI and Visualization           |         33.38 |       15.5  |      115.16 |   365.97 |   529.99 |\n| Data Analysis                  |        119.15 |       55.32 |      411.1  |  1306.46 |  1891.96 |\n| Data Architecture and Modeling |         23.76 |       11.03 |       81.97 |   260.5  |   377.24 |\n| Data Engineering               |        175.39 |       81.43 |      605.14 |  1923.09 |  2784.94 |\n| Data Management                |         10.97 |        5.09 |       37.86 |   120.32 |   174.25 |\n| Data Science and Research      |        260.48 |      120.93 |      898.69 |  2855.98 |  4135.92 |\n| Leadership and Management      |         43.14 |       20.03 |      148.84 |   473    |   684.99 |\n| Machine Learning and AI        |        113.99 |       52.92 |      393.29 |  1249.84 |  1809.96 |\n| All                            |        780.23 |      362.24 |     2691.95 |  8554.83 | 12388.8  |\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# save final contingency table for use in analyses\ntrans_refined_ct = refined_ct.T\n```\n:::\n\n\n### 3.B: Random Forest Regressor Model Preparation\n\n#### 3.B.1: Encoding Section\n\nSince scikit-learn's RFR implementation can only accept numeric features, predictors for the RFR model (`experience_level`, `work_setting`, and `job_category`) were transformed using a one-hot encoding (OHE) method. While added sparsity from OHE features can reduce model efficiency and performance [@ravi2022], other encoding methods that do not increase sparsity (e.g., integer or ordinal encoding) can create unintended relationships between feature levels [@brownlee2020]. Such ordered relationships would have been defensible for features `experience_level` and `work_setting`, but these were also encoded using an OHE method for consistency with the `job_category` feature. By using the `ColumnTransformer()` function to apply OHE, the target variable, `salary_in_usd`, was left unaffected, and the output was automatically transformed into a pandas dataframe. This resulted in seventeen OHE features, corresponding to the levels of `experience_level` (*n* = 4), `work_setting` (*n* = 3), and `job_category` (*n* = 10).\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# import libraries necessary for transformation\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n```\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# get list of categorical columns to transform\ncat_df = refined_us_job_df[['experience_level', 'work_setting', 'job_category']]\ncat_cols = list(cat_df.keys())\nprint(cat_cols)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['experience_level', 'work_setting', 'job_category']\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# define transformer with non-sparse output\ncat_transformer = OneHotEncoder(sparse_output=False)\n```\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# apply transformer to cat_cols, while passing over salary_in_usd\nct = ColumnTransformer([('cat', cat_transformer, cat_cols)], remainder='passthrough')\n```\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# set output to pandas df and fit to data\nct.set_output(transform='pandas')\ndc_us_job_df = ct.fit_transform(refined_us_job_df)\ndc_us_job_df.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nIndex(['cat__experience_level_Entry-level', 'cat__experience_level_Executive',\n       'cat__experience_level_Mid-level', 'cat__experience_level_Senior',\n       'cat__work_setting_Hybrid', 'cat__work_setting_In-person',\n       'cat__work_setting_Remote', 'cat__job_category_BI and Visualization',\n       'cat__job_category_Cloud and Database',\n       'cat__job_category_Data Analysis',\n       'cat__job_category_Data Architecture and Modeling',\n       'cat__job_category_Data Engineering',\n       'cat__job_category_Data Management and Strategy',\n       'cat__job_category_Data Quality and Operations',\n       'cat__job_category_Data Science and Research',\n       'cat__job_category_Leadership and Management',\n       'cat__job_category_Machine Learning and AI',\n       'remainder__salary_in_usd'],\n      dtype='object')\n```\n:::\n:::\n\n\n#### 3.B.2: Feature Selection\n\nOnce the features were all in a numeric format, scikit-learn's `SelectKBest()` discerned which variables were significantly associated with salary (see @tbl-feat-select for a summary). The scoring function used was `f_regression()`, due to the target feature being continuous. Three features were not significantly associated with `salary_in_usd` at the *p* \\< .05 level (`cat__work_setting_Hybrid`, `cat__job_category_Data Architecture and Modeling`, `cat__job_category_Cloud and Database`). These features were dropped from subsequent analyses, leaving a total of fourteen features.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n# import libraries for feature selection\nfrom sklearn.feature_selection import SelectKBest, f_regression\n```\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# create X and y arrays for feature selection and beyond\ny = dc_us_job_df['remainder__salary_in_usd']\nX = dc_us_job_df.drop(['remainder__salary_in_usd'], axis=1)\nprint(f'''X shape: {X.shape}, y shape: {y.shape}''')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX shape: (12389, 17), y shape: (12389,)\n```\n:::\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# create SelectKBest instance using f_regression on all features\nfeature_rank = SelectKBest(score_func=f_regression, k='all')\n# fit to data\nfeature_rank.fit(X, y=y)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelectKBest(k=&#x27;all&#x27;, score_func=&lt;function f_regression at 0x15b8caf20&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest(k=&#x27;all&#x27;, score_func=&lt;function f_regression at 0x15b8caf20&gt;)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {#tbl-feat-select .cell tbl-cap='Feature importances ranked by significance.' execution_count=27}\n``` {.python .cell-code}\n# create df to show features, F-scores, and p-values\nfeature_df = pd.DataFrame({'Feature': X.columns, \n                           'F-score': feature_rank.scores_.round(2), \n                           'P-value': feature_rank.pvalues_.round(3)})\n# sort lowest p-values first \nfeature_df = feature_df.sort_values(by='P-value', ascending=True)\ndisplay(Markdown(feature_df.to_markdown()))\n```\n\n::: {.cell-output .cell-output-display}\n|    | Feature                                          |   F-score |   P-value |\n|---:|:-------------------------------------------------|----------:|----------:|\n|  0 | cat__experience_level_Entry-level                |    643.87 |     0     |\n| 14 | cat__job_category_Data Science and Research      |    321.51 |     0     |\n| 13 | cat__job_category_Data Quality and Operations    |     53.27 |     0     |\n| 12 | cat__job_category_Data Management and Strategy   |     97.05 |     0     |\n|  9 | cat__job_category_Data Analysis                  |   1323    |     0     |\n| 15 | cat__job_category_Leadership and Management      |     23.38 |     0     |\n|  7 | cat__job_category_BI and Visualization           |     89.74 |     0     |\n| 16 | cat__job_category_Machine Learning and AI        |    791.71 |     0     |\n|  5 | cat__work_setting_In-person                      |     23.33 |     0     |\n|  3 | cat__experience_level_Senior                     |    838.27 |     0     |\n|  2 | cat__experience_level_Mid-level                  |    509.32 |     0     |\n|  1 | cat__experience_level_Executive                  |    169.43 |     0     |\n|  6 | cat__work_setting_Remote                         |     21.73 |     0     |\n| 11 | cat__job_category_Data Engineering               |     10.23 |     0.001 |\n|  4 | cat__work_setting_Hybrid                         |      2.18 |     0.14  |\n| 10 | cat__job_category_Data Architecture and Modeling |      1.64 |     0.2   |\n|  8 | cat__job_category_Cloud and Database             |      0.58 |     0.446 |\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# drop non-significant p-values from features\nX = X.drop(['cat__work_setting_Hybrid', \n            'cat__job_category_Data Architecture and Modeling', \n            'cat__job_category_Cloud and Database'], axis=1)\n```\n:::\n\n\n#### 3.B.3: Train-Test-Split\n\nFollowing the conventions of machine learning best practices, the data for the RFR model was split into training and testing sets, with a ratio of 70:30 in favor of the training set. This ensures the RFR model can generalize beyond the data it trained on (i.e., to the testing set), since this resembles the type of predictive modeling paradigm wherein the predicted values are not available [@brownlee2020c]. This 70:30 split resulted in training sets with 8,672 observations and testing sets with 3,717 observations.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# create train and test arrays from X and y with 80% train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, \n                                                    random_state=21)\n```\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n# output dimensions of train and test sets \nprint(f'''X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, \n      y_train shape: {y_train.shape}, y_test shape: {y_test.shape}''')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX_train shape: (8672, 14), X_test shape: (3717, 14), \n      y_train shape: (8672,), y_test shape: (3717,)\n```\n:::\n:::\n\n\n## 4: Analysis\n\n### 4.A: Chi-Square Model\n\n#### 4.A.1: Omnibus\n\nThe omnibus-level Chi-Square Test of Independence was statistically significant (*X^2^* = 1003.54, *p* \\< .001). Thus, the null hypothesis was rejected, as the experience level composition of employees varied between job categories. This necessitated further post hoc testing to compare experience level composition between job categories.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# output omnibus Chi-Square result\nfrom scipy.stats import chi2_contingency\nresult = chi2_contingency(trans_refined_ct)\nprint(f'''Statistic: {result.statistic}, P-value: {result.pvalue}''')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatistic: 1003.5413047409224, P-value: 4.926315041672239e-199\n```\n:::\n:::\n\n\n#### 4.A.2: Post Hoc Tests\n\nThe post hoc tests were completed using code adapted from @neuhof2018, as this helped streamline the process for the 28 job category comparisons. Instead of printing these comparisons as in the original code, converting to and printing a dataframe yielded a cleaner output (see @tbl-chi-sig). Given the large dataset at hand, a more stringent Bonferroni correction was used to correct for alpha inflation [@jafari2019], as this method multiples the statistical significance by the number of comparisons made. Despite this, all 28 comparisons were statistically significant after the Bonferroni correction (Bonferroni-corrected *p*-range: *p* \\< .001 - *p* = .001). Focusing instead on the magnitude of the effect size in terms of *X^2^*, the three strongest effects resulted from comparisons with Data Analysis (compared to Data Engineer, Machine Learning and AI, and Data Science and Research, respectively). The visualizations in the following suggest that Data Analysis had more Entry-level employees relative to the comparison categories.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\n# code adapted from Neuhof (2018)\nfrom itertools import combinations\n\n# gathering all combinations for post-hoc chi2\nall_combinations = list(combinations(trans_refined_ct.index, 2))\nchi2_vals = []\np_vals = []\nfor comb in all_combinations:\n    # subset df into a dataframe containing only the pair \"comb\"\n    new_df = trans_refined_ct[(trans_refined_ct.index == comb[0]) | \n                              (trans_refined_ct.index == comb[1])]\n    # running chi2 test\n    chi2, p, dof, ex = chi2_contingency(new_df, correction=True)\n    chi2_vals.append(chi2)\n    p_vals.append(p)\n```\n:::\n\n\n::: {#tbl-chi-sig .cell tbl-cap='Job category comparisons ranked by Chi-Square statistic.' execution_count=33}\n``` {.python .cell-code}\n# create dataframe with combinations, effect sizes, and p-vals\nchi2_df = pd.DataFrame({'Combination': all_combinations, \n                        'Statistic': chi2_vals, \n                        'p': p_vals})\n# create Bonferroni-corrected p-values by multiplying by number of tests\nchi2_df['p-Bonf'] = (chi2_df['p'] * 28).round(3)\nchi2_df['Statistic'] = chi2_df['Statistic'].round(2)\nchi2_df['p'] = chi2_df['p'].round(3)\n# sort df by effect size \nchi2_df = chi2_df[['Combination', 'Statistic', 'p', 'p-Bonf']].sort_values(\nby='Statistic', ascending=False)\n# print as markdown\ndisplay(Markdown(chi2_df.to_markdown()))\n```\n\n::: {.cell-output .cell-output-display}\n|    | Combination                                                     |   Statistic |   p |   p-Bonf |\n|---:|:----------------------------------------------------------------|------------:|----:|---------:|\n|  8 | ('Data Analysis', 'Data Engineering')                           |      340.85 |   0 |    0     |\n| 12 | ('Data Analysis', 'Machine Learning and AI')                    |      325.68 |   0 |    0     |\n| 10 | ('Data Analysis', 'Data Science and Research')                  |      297.49 |   0 |    0     |\n| 11 | ('Data Analysis', 'Leadership and Management')                  |      199.27 |   0 |    0     |\n| 27 | ('Leadership and Management', 'Machine Learning and AI')        |      170.07 |   0 |    0     |\n|  7 | ('Data Analysis', 'Data Architecture and Modeling')             |      156.55 |   0 |    0     |\n| 14 | ('Data Architecture and Modeling', 'Data Management')           |      149.71 |   0 |    0     |\n| 24 | ('Data Management', 'Machine Learning and AI')                  |      140.46 |   0 |    0     |\n| 25 | ('Data Science and Research', 'Leadership and Management')      |      127.48 |   0 |    0     |\n| 16 | ('Data Architecture and Modeling', 'Leadership and Management') |      122.45 |   0 |    0     |\n|  1 | ('BI and Visualization', 'Data Architecture and Modeling')      |       96.26 |   0 |    0     |\n| 18 | ('Data Engineering', 'Data Management')                         |       86.38 |   0 |    0     |\n|  6 | ('BI and Visualization', 'Machine Learning and AI')             |       84.62 |   0 |    0     |\n| 21 | ('Data Engineering', 'Machine Learning and AI')                 |       75.22 |   0 |    0     |\n| 22 | ('Data Management', 'Data Science and Research')                |       73.34 |   0 |    0     |\n| 13 | ('Data Architecture and Modeling', 'Data Engineering')          |       70.68 |   0 |    0     |\n| 15 | ('Data Architecture and Modeling', 'Data Science and Research') |       63.27 |   0 |    0     |\n| 23 | ('Data Management', 'Leadership and Management')                |       57.52 |   0 |    0     |\n| 26 | ('Data Science and Research', 'Machine Learning and AI')        |       56.39 |   0 |    0     |\n|  0 | ('BI and Visualization', 'Data Analysis')                       |       53    |   0 |    0     |\n| 20 | ('Data Engineering', 'Leadership and Management')               |       50.03 |   0 |    0     |\n| 19 | ('Data Engineering', 'Data Science and Research')               |       39.92 |   0 |    0     |\n|  9 | ('Data Analysis', 'Data Management')                            |       30.91 |   0 |    0     |\n|  5 | ('BI and Visualization', 'Leadership and Management')           |       29.99 |   0 |    0     |\n|  2 | ('BI and Visualization', 'Data Engineering')                    |       27    |   0 |    0     |\n|  3 | ('BI and Visualization', 'Data Management')                     |       25.48 |   0 |    0     |\n|  4 | ('BI and Visualization', 'Data Science and Research')           |       22.98 |   0 |    0.001 |\n| 17 | ('Data Architecture and Modeling', 'Machine Learning and AI')   |       22.92 |   0 |    0.001 |\n:::\n:::\n\n\n#### 4.A.3: Chi-Square Visualizations\n\nThe contingency cross tabulation tables are shown below for the observed (see @tbl-ct-figs-1) and expected counts (see @tbl-ct-figs-2). Rather than focusing on these individually, a residual table was created to show observed counts deviated from the expected counts for each cell (see @tbl-ct-figs-3). Residuals were calculated as the difference between the observed and expected counts divided by the expected values ((O - E) / E), which yields both the direction and magnitude of difference between the two. The accompanying heatmap (see @fig-chi-heatmap) shows where counts were greater than expected in red (e.g., Entry-level and Data Analysis or Executive and Leadership and Management) and fewer than expected in blue (e.g., Entry-level and Data Architecture and Modeling or Executive and Data Management). The bivariate plots compare the experience level composition across job categories, using the proportion (@fig-chi-bivariate-1) and count of employees (@fig-chi-bivariate-2) with each experience level. @fig-chi-bivariate-1 re-iterates the findings that Data Analysis for Entry-level employees and Leadership and Management for Executive employees were relative peaks among less-represented experience levels. Conversely, Senior employees comprised a sizable portion of every job category, and the clear majority of a few (e.g., Data Architecture and Modeling, Machine Learning and AI). @fig-chi-bivariate-2 uses number of employees for the y-axis variable, which takes into account size differences between job categories. This again depicts the relative commonality of Senior employees, as three of four largest categories (Data Science and Research, Data Engineering, and Machine Learning and AI) contained a substantial number of Senior employees.\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# set order of Chi-Square variables\n# set experience order\nexperience_order = pd.CategoricalDtype(['Entry-level', 'Mid-level', 'Senior', \n                                        'Executive'], ordered=True)\nct_us_job_df.experience_level = ct_us_job_df.experience_level.astype(experience_order)\n# set job category order to alphabetical\ncategory_order = pd.CategoricalDtype(['BI and Visualization', 'Data Analysis', \n                                      'Data Architecture and Modeling', \n                                      'Data Engineering', 'Data Management', \n                                      'Data Science and Research', \n                                      'Leadership and Management', \n                                      'Machine Learning and AI'], ordered=True)\nct_us_job_df.job_category = ct_us_job_df.job_category.astype(category_order)\n```\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n# create crosstabs without margins\nobserved_ct = pd.crosstab(ct_us_job_df['experience_level'],\n                          ct_us_job_df['job_category'], margins=False)\nexpected_ct = sm.stats.Table(observed_ct)\n```\n:::\n\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\n# convert cts to dfs and transpose\nexpected_df = pd.DataFrame(expected_ct.fittedvalues).T\nobserved_df = pd.DataFrame(observed_ct).T\n```\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\n# run transformations to create residuals \nresid_div_ct_df = observed_df.subtract(expected_df)\nresid_div_ct_df = resid_div_ct_df.divide(expected_df)\n```\n:::\n\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\n# round all dfs for better outputting\nexpected_df = expected_df.round(2)\nobserved_df = observed_df.round(2)\nresid_div_ct_df = resid_div_ct_df.round(2)\n```\n:::\n\n\n::: {#tbl-ct-figs .cell .column-body-outset tbl-subcap='[\"Observed counts.\",\"Expected counts.\",\"Residual counts.\"]' layout='[[65,35],[100]]' tbl-cap='Contingency Tables' execution_count=39}\n``` {.python .cell-code}\n# display tables in markdown format\ndisplay(Markdown(observed_df.to_markdown()))\ndisplay(Markdown(expected_df.to_markdown(index=False)))\ndisplay(Markdown(resid_div_ct_df.to_markdown()))\n```\n\n::: {.cell-output .cell-output-display}\n| job_category                   |   Entry-level |   Mid-level |   Senior |   Executive |\n|:-------------------------------|--------------:|------------:|---------:|------------:|\n| BI and Visualization           |            37 |         150 |      325 |          18 |\n| Data Analysis                  |           347 |         450 |     1075 |          20 |\n| Data Architecture and Modeling |             0 |          34 |      339 |           4 |\n| Data Engineering               |            95 |         623 |     1937 |         130 |\n| Data Management                |            22 |          74 |       78 |           0 |\n| Data Science and Research      |           216 |         866 |     2954 |         100 |\n| Leadership and Management      |            16 |         216 |      393 |          60 |\n| Machine Learning and AI        |            47 |         279 |     1454 |          30 |\n:::\n\n::: {.cell-output .cell-output-display}\n|   Entry-level |   Mid-level |   Senior |   Executive |\n|--------------:|------------:|---------:|------------:|\n|         33.39 |      115.15 |   365.95 |       15.51 |\n|        119.19 |      411.08 |  1306.38 |       55.36 |\n|         23.78 |       82.02 |   260.65 |       11.04 |\n|        175.44 |      605.1  |  1922.98 |       81.48 |\n|         10.99 |       37.91 |   120.49 |        5.11 |\n|        260.54 |      898.64 |  2855.81 |      121.01 |\n|         43.15 |      148.83 |   472.98 |       20.04 |\n|        114.02 |      393.26 |  1249.76 |       52.96 |\n:::\n\n::: {.cell-output .cell-output-display}\n| job_category                   |   Entry-level |   Mid-level |   Senior |   Executive |\n|:-------------------------------|--------------:|------------:|---------:|------------:|\n| BI and Visualization           |          0.11 |        0.3  |    -0.11 |        0.16 |\n| Data Analysis                  |          1.91 |        0.09 |    -0.18 |       -0.64 |\n| Data Architecture and Modeling |         -1    |       -0.59 |     0.3  |       -0.64 |\n| Data Engineering               |         -0.46 |        0.03 |     0.01 |        0.6  |\n| Data Management                |          1    |        0.95 |    -0.35 |       -1    |\n| Data Science and Research      |         -0.17 |       -0.04 |     0.03 |       -0.17 |\n| Leadership and Management      |         -0.63 |        0.45 |    -0.17 |        1.99 |\n| Machine Learning and AI        |         -0.59 |       -0.29 |     0.16 |       -0.43 |\n:::\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\n# show residuals using heatmap using blue and red colors, centering on 0\nresid_heatmap = sns.heatmap(resid_div_ct_df, annot=True, cmap='coolwarm', center=0)\nresid_heatmap.set(xlabel='Experience Level', ylabel='Job Category')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Chi-Square residual heatmap. Positive values denote greater observed counts than expected in red. Negative values denoting fewer observed counts than expected are shown in blue.](capstone_report_files/figure-html/fig-chi-heatmap-output-1.png){#fig-chi-heatmap width=762 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\n# reset experience_order to show experience vertically\nexperience_order = pd.CategoricalDtype(['Executive', 'Senior', 'Mid-level', \n                                        'Entry-level'], ordered=True)\nct_us_job_df.experience_level = ct_us_job_df.experience_level.astype(experience_order)\n```\n:::\n\n\n::: {#fig-chi-bivariate .cell .column-body-outset layout-ncol='2' execution_count=42}\n``` {.python .cell-code}\n# show bivariate plots for experience level and job category proportions and counts\nwith sns.color_palette('colorblind'):\n    prop_hist = sns.histplot(data=ct_us_job_df, x='job_category', \n                             hue='experience_level', stat='proportion', \n                             multiple='fill', discrete=True)\n    prop_hist.set_xticklabels(['BI/Viz', 'DA', 'DArc/Modeling', 'DE', 'DM', \n                               'DS/Research', 'Leadership', 'ML/AI'], rotation=45)\n    prop_hist.set(xlabel='Job Category', ylabel='Proportion of Employees')#, \n                  #title='Experience Level Composition by Job Category')\n    prop_hist.get_legend().set_title('Experience Level')\n    plt.show()\n\n    count_hist = sns.histplot(data=ct_us_job_df, x='job_category', \n                              hue='experience_level', multiple='stack')\n    count_hist.set_xticklabels(['BI/Viz', 'DA', 'DArc/Modeling', 'DE', 'DM', \n                                'DS/Research', 'Leadership', 'ML/AI'], rotation=45)\n    count_hist.set(xlabel='Job Category', ylabel='Number of Employees')\n    count_hist.get_legend().set_title('Experience Level')\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Proportion of job category with each experience level.](capstone_report_files/figure-html/fig-chi-bivariate-output-1.png){#fig-chi-bivariate-1 width=589 height=499}\n:::\n\n::: {.cell-output .cell-output-display}\n![Number of employees with experience level across job categories.](capstone_report_files/figure-html/fig-chi-bivariate-output-2.png){#fig-chi-bivariate-2 width=602 height=494}\n:::\n\nExperience level and job category composition. Abbreviations: BI/Viz: BI and Visualization; DA: Data Analysis; DArc/Modeling: Data Architecture and Modeling; DE: Data Engineering; DM: Data Management; DS/Research: Data Science and Research; Leadership: Leadership and Management; ML/AI: Machine Learning and AI.\n:::\n\n\n### 4.B: Random Forest Regressor Model\n\n#### 4.B.1: Grid Search for RFR\n\nTo create a more robust RFR model, hyperparameters of interest were tuned using `GridSearchCV` to evaluate hyperparameter combinations with the training data. The parameter grid was constructed using guidelines from @ellis2022, specifying values in a reasonable range for `n_estimators`, `max_features`, and one parameter to limit tree depth (e.g., `max_depth`). With root-mean-squared error (*RMSE*) as the a-priori error metric for the RFR model, the scoring metric for the grid search object was set to `neg_root_mean_squared_error`. This grid search object was supplied a default RFR model and the parameter grid, and subsequently fit to the `X_train` and `y_train` datasets for five cross-validation folds. The resulting best parameters were `max_depth` = 8, `max_features` = 'sqrt', and `n_estimators` = 50, which were used with the models in the next section (see section [4.B.2: Train and Test Model Performance]).\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\n# import libraries necessary for hyperparameter tuning\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n```\n:::\n\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\n# define MSE for later use\nMSE = metrics.mean_squared_error\n```\n:::\n\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\n# instantiate baseline model\nrf_model = RandomForestRegressor(random_state=21)\n```\n:::\n\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\n# construct parameter grid to for hyperparameters of interest\nparam_grid = {'n_estimators': [50, 100, 200, 400], \n              'max_depth': [2, 4, 8, None],\n              'max_features': ['sqrt', 'log2', None]\n              }\n```\n:::\n\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\n# fit GridSearchCV to rf_model with param_grid and scoring as negative RMSE\ngs_rf = GridSearchCV(rf_model, param_grid=param_grid, \n                     scoring='neg_root_mean_squared_error', cv=5)\n```\n:::\n\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\n# fit grid search object to data\ngs_rf.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```{=html}\n<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=21),\n             param_grid={&#x27;max_depth&#x27;: [2, 4, 8, None],\n                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;, None],\n                         &#x27;n_estimators&#x27;: [50, 100, 200, 400]},\n             scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=21),\n             param_grid={&#x27;max_depth&#x27;: [2, 4, 8, None],\n                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;, None],\n                         &#x27;n_estimators&#x27;: [50, 100, 200, 400]},\n             scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=21)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=21)</pre></div></div></div></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\n# output best parameters from grid search\nprint(gs_rf.best_params_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 50}\n```\n:::\n:::\n\n\n#### 4.B.2: Train and Test Model Performance\n\nThe training model performance was evaluated from the grid search object directly, from which the *RMSE* and *R^2^* values were extracted. The final RFR model was created with tuned hyperparameters and fit to `X_train` and `y_train`, and subsequently used to predict `y_test` using `X_test`. Model performance was comparable in the training (*RMSE* = 53,050.72, *R^2^* = .2448) and testing models (*RMSE* = 54,352.70, *R^2^* = .2525). The testing model performance indicates that the features explained roughly 25% of the variance in salary, and the salary estimates were off by roughly \\$54,000 on average. The convergence between the error and explained variance metrics between the training and testing models suggest the model was not overfit to the training data.\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\n# get RMSE and R2 from best grid search model (training)\ngrid_rmse = (gs_rf.best_score_ * -1)\nprint('Training RMSE: ', round(grid_rmse, 2))\ngrid_predict = gs_rf.predict(X_train)\nprint('Training R-Squared: ', round(metrics.r2_score(y_true=y_train, \n      y_pred=grid_predict), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining RMSE:  53050.72\nTraining R-Squared:  0.2448\n```\n:::\n:::\n\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\n# define final RFR model with best parameters\nrfr_model = RandomForestRegressor(max_depth=8, \n                                  max_features='sqrt', \n                                  n_estimators=50, \n                                  random_state=21)\n```\n:::\n\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\n# fit final model to X_train and y_train\nrfr_model.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```{=html}\n<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, n_estimators=50,\n                      random_state=21)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, n_estimators=50,\n                      random_state=21)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\n# using that RFR model, predict y_test using X_test\ny_test_pred = rfr_model.predict(X_test)\n```\n:::\n\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\n# get RMSE and R2 values from test model\ntest_RMSE = np.sqrt(MSE(y_test, y_test_pred))\ntest_R2 = metrics.r2_score(y_test, y_test_pred)\nprint('Test RMSE: ', round(test_RMSE, 2))\nprint('Test R-Squared: ', round(test_R2, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest RMSE:  54352.7\nTest R-Squared:  0.2525\n```\n:::\n:::\n\n\n#### 4.B.3: Feature Importances and Tree Diagram\n\nFeature importances were extracted from the RFR model, converted to a dataframe, and sorted in descending order (see @tbl-feature-imps). The values ranged from .008 to .286, with all features explaining at least some variance, and the magnitude gradually decreasing with each subsequent feature (see @fig-feat-imp). The top three features included the Data Analysis job category (.286), Machine Learning and AI job category (.162), and Senior experience level (.136). Unsurprisingly, these were the first three features used to split the data in the RFR tree diagram (see @fig-tree-three), which underscores each feature's importance to estimating salary. The code for this diagram was adapted from a TensorFlow tutorial [@visualiz2024], and this figure makes it easy to interpret how the absence or presence of an experience level, job category, or work setting altered the RFR model's salary estimate. With the depth of the model, however, only the first three levels of the diagram could be displayed before the clarity of the figure diminished.\n\n::: {#tbl-feature-imps .cell tbl-cap='Sorted feature importances in Random Forest model.' execution_count=55}\n``` {.python .cell-code}\n# create df for features and corresponding importances \nfeature_df = pd.DataFrame({'Feature': X.columns, \n                           'Importance': rfr_model.feature_importances_})\n# sort the values with higher importances first\nfeature_df['Importance'] = feature_df['Importance'].round(3)\nfeature_df = feature_df.sort_values(by='Importance', ascending=False)\n# display sorted df as markdown\ndisplay(Markdown(feature_df.to_markdown()))\n```\n\n::: {.cell-output .cell-output-display}\n|    | Feature                                        |   Importance |\n|---:|:-----------------------------------------------|-------------:|\n|  7 | cat__job_category_Data Analysis                |        0.286 |\n| 13 | cat__job_category_Machine Learning and AI      |        0.162 |\n|  3 | cat__experience_level_Senior                   |        0.136 |\n|  0 | cat__experience_level_Entry-level              |        0.082 |\n|  2 | cat__experience_level_Mid-level                |        0.082 |\n| 11 | cat__job_category_Data Science and Research    |        0.079 |\n|  1 | cat__experience_level_Executive                |        0.056 |\n|  6 | cat__job_category_BI and Visualization         |        0.028 |\n|  8 | cat__job_category_Data Engineering             |        0.022 |\n|  9 | cat__job_category_Data Management and Strategy |        0.02  |\n|  5 | cat__work_setting_Remote                       |        0.015 |\n|  4 | cat__work_setting_In-person                    |        0.013 |\n| 12 | cat__job_category_Leadership and Management    |        0.012 |\n| 10 | cat__job_category_Data Quality and Operations  |        0.008 |\n:::\n:::\n\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\n# create split version of encoded feature names\nfeature_df['sep_Variable'] = feature_df['Feature'].str.split(r'_', regex=True,\n                                                              expand=False)\n# take last portion of the split for cleaner feature names\nfeature_df['short_Variable'] = feature_df['sep_Variable'].str[-1]\n```\n:::\n\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\n# visualize feature importances in descending order \nwith sns.color_palette('colorblind'):\n    feature_bar = sns.barplot(data=feature_df, x='Importance', y='short_Variable')\n    feature_bar.set(xlabel='Impurity-based Feature Importance', ylabel='Feature')\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Feature importances in Random Forest model.](capstone_report_files/figure-html/fig-feat-imp-output-1.png){#fig-feat-imp width=796 height=429}\n:::\n:::\n\n\n::: {.cell .column-body execution_count=58}\n``` {.python .cell-code}\n# code adapted from Visualizing TensorFlow Decision Trees with Dtreeviz (2024)\nimport dtreeviz\n\n# create dtreeviz model from RFR model\nviz_rfr_model = dtreeviz.model(rfr_model.estimators_[0], X, y, \n                               tree_index=3,\n                               feature_names=feature_df['short_Variable'], \n                               target_name='Salary')\n# only show first three levels of depth for sake of clarity\nviz_rfr_model.view(depth_range_to_display=[0,2], scale=1.2)\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n![Tree diagram from Random Forest model. Only the first three levels are shown to preserve figure clarity.](capstone_report_files/figure-html/fig-tree-three-output-1.svg){#fig-tree-three}\n:::\n:::\n\n\n## 5: Data Summary and Implications\n\nThe results of the Chi-Square Test of Independence were statistically significant (see section [4.A.1: Omnibus]), thus rejecting the null hypothesis that the experience level of employees did not vary between job categories (see section [1.1: Research Question and Hypotheses]). With regards to this study's research question, experience level composition varied between job categories, which suggests that some data science positions are typically held by more experienced employees than others. One limitation of using a Chi-Square test with this data was that it may have been overpowered, as all job category post hoc comparisons exhibited significant differences, even following a Bonferroni correction (see section [4.A.2: Post Hoc Tests]). This shifted the focus from statistical significance of the comparisons to the magnitude of the effects, as quantified by the *X^2^* values and the residuals, (see section [4.A.3: Chi-Square Visualizations] for visualizations). Both metrics converged to suggest the Data Analysis job category was unique in its distribution of experience levels, which featured the highest proportion of entry-level employees.\n\nWhile the RFR model was not intended to test or evaluate hypotheses, the final model performed relatively well considering the challenges of the data (see section [4.B.2: Train and Test Model Performance]). The features accounted for more than one-quarter of the variance in salary, which is impressive considering the fourteen encoded features originated from only three features. One limitation described in section [3.B.1: Encoding Section] was the use of sparse categorical data in the RFR model, which has been shown to diminish performance or interpretability in some Random Forest models [@ravi2022]. Interestingly, the RFR testing model performance (*R^2^*) outperformed the training model slightly (.2525 compared to .2448), which was unexpected. This can likely be attributed to the idiosyncratic, random nature of the train-test-split function (see section [3.B.3: Train-Test-Split]), which may have given the testing set a slightly \"easier\" dataset to predict. Since the training model had a better *RMSE* value than the testing set (53,050.72 compared to 54,352.70), the training model still estimated salary better than the testing set, and the unexpected *R^2^* comparison is likely not concerning.\n\nThe results of this study may be helpful for characterizing the field of data science at large. Firstly, the analyses using the data at hand suggest that entry-level positions are relatively rare in data science. This may lend credence to the idea that fewer entry-level positions are available in the field [@selvaraj2022], although, further studies should investigate this claim. The relative spike of entry-level employees in the data analysis job category supports multiple resources cited here [@selvaraj2022; @simmons2023]. Each of these suggested a data analyst position may be an intermediate step for an entry-level employee to become a data scientist, and the results of this study suggest this may be plausible. Lastly, the results of the RFR model suggest some experience levels, job categories, and work settings tend to earn a higher salary than others. Notably, employees in the Data Analysis job category tended to earn less, whereas Senior employees, or employees in the Machine Learning and AI job category tended to earn more.\n\nFour considerations for future studies examining the field of data science are recommended following analyses here. One technical improvement would be to use an RFR model with non-sparse categorical features, to see if this potentially improves model performance or interpretability. An alternative to the scikit-learn workflow is the Distributed Random Forest model from @distribu2024, which can incorporate nominal feature structures without requiring transformations. Concerning the association between experience level and job category, obtaining each employee's entry-level position to data science would help create more holistic, trajectory-based snapshots of data science careers. This could also disentangle whether some roles with higher proportions of entry-level employees (e.g., Data Analysis) could feasibly serve as an intermediate step to another position [@selvaraj2022; @simmons2023]. Concerning the RFR model, including more features that might be associated with data science salaries (e.g., education level, years of employment, or geographic location) could improve salary estimates and model performance. Lastly, acquiring more stratified data with respect to employee status, employee residence, and company location would help generalize analyses beyond full-time employees, residing in the U.S., and working for a U.S.-based company.\n\n\n{{< pagebreak >}}\n\n\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "capstone_report_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}