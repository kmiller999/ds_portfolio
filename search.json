[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin Miller",
    "section": "",
    "text": "Email\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \n\nAbout Me\nI am an end-to-end Data Scientist with a versatile skill set that spans statistical analysis, data modeling, machine learning, and modern data engineering. I am passionate about working across the full data lifecycle: from ingestion and transformation to modeling, deployment, and insight.\nMy work is driven by a passion for uncovering patterns in complex data and building systems that scale. While I continue to deepen my skills in data infrastructure and engineering, I remain committed to asking and answering novel questions through advanced data mining techniques.\nTo learn more about me or get in touch, please contact me through email or connect with me on LinkedIn.\n\n\n\n\n\nProgramming Languages\n\n\n\nPython (Examples)\nPandas, Numpy, Scikit-learn, statsmodels\n\n\nR (Examples)\ntidyverse, afex\n\n\n\nSQL (Examples)\nSQL Server, PostgreSQL, SQLite\n\n\nLinux\nTerminal navigation, script and batch job deployment\n\n\n\n\n\nSkills and Interests\n\nCurrent Strengths\n\nInferential Statistical Modeling: bivariate and multivariate ANOVA and Regression (linear and logistic)\nTree-Based Models: Decision Trees and Random Forest Models (Examples)\nDimensionality Reduction: Principal Component Analysis (PCA; Examples)\nWeb Scraping: BeautifulSoup and Selenium in Python (Examples)\nMarket Basket Analysis: Support, Confidence, and Lift (Examples)\nData Visualization and Reporting: Power BI and Tableau dashboards (Examples)\n\n\n\nActively Developing\n\nBuilding advanced ETL workflows using APIs and Python scripting\nIntegrating disparate data sources into analyzable, well-structured formats\nDesigning scalable, relational database models using SQL\nEngineering features from complex datasets to model nuanced constructs\nCharacterizing and predicting real-world decision-making behaviors"
  },
  {
    "objectID": "posts/capstone_report/code/capstone_report.html",
    "href": "posts/capstone_report/code/capstone_report.html",
    "title": "Chi-Square and Random Forest Regression with Data Science Jobs Dataset",
    "section": "",
    "text": "This work represents a finalized Capstone Report for my Master’s of Science in Data Analytics from Western Governors University (WGU). To comply with WGU’s academic integrity guidelines, the present document was not submitted as part of my Capstone. Rather, this report represents an extension of the project that showcases my work without making my submitted materials publicly available."
  },
  {
    "objectID": "posts/capstone_report/code/capstone_report.html#abstract",
    "href": "posts/capstone_report/code/capstone_report.html#abstract",
    "title": "Chi-Square and Random Forest Regression with Data Science Jobs Dataset",
    "section": "Abstract",
    "text": "Abstract\nThe growth of data science as a field has produced simultaneous increases in role diversification. The present study further characterized differences between job roles by examining whether experience levels varied between data science job categories. This study focused on a subset of publicly available data comprising data science employees working full-time in the U.S. (N = 12,389). A Chi-Square Test of Independence with subsequent post hoc tests analyzed the association between experience level and job category. A Random Forest Regression model was used to predict salaries from experience level, job category, and work setting. Experience level composition varied between job categories (X2 = 1003.54, p &lt; .001), with each job category differing from all others (p-range: p &lt; .001 - p = .001). Residual analysis revealed two experience level-job category intersections with substantially greater observed counts than expected: executive employees in leadership roles and entry-level employees in data analysis roles. Employees in these experience levels were otherwise underrepresented, whereas senior employees comprised a sizable portion of each job category. The Random Forest Regression model explained roughly 25% of the variance in employee salary. Salary estimates were higher for employees in machine learning or AI-related roles or senior positions, and lower for employees in data analysis roles. This study further characterized differences between data science roles, most notably for experience levels typically found in each. These findings have implications for the accessibility of these roles, particularly for entry-level employees, who may have fewer attainable job roles."
  },
  {
    "objectID": "posts/capstone_report/code/capstone_report.html#introduction-and-research-question",
    "href": "posts/capstone_report/code/capstone_report.html#introduction-and-research-question",
    "title": "Chi-Square and Random Forest Regression with Data Science Jobs Dataset",
    "section": "1: Introduction and Research Question",
    "text": "1: Introduction and Research Question\nAfter being heralded as the “sexiest job of the 21st century” (Davenport & Patil, 2012), the data scientist role met these lofty expectations (Davenport & Patil, 2022), and is expected to continue growing through 2032 (U.S. Bureau of Labor Statistics, 2024). The responsibilities of this role grew simultaneously, requiring more specialized data science roles be created to focus on a portion of the data science process (Davenport & Patil, 2022). Examples of these expanded roles include the data engineer and data analyst (Rosidi, 2023), with the former focusing on the quality and accessibility of data and the latter focusing on analysis and reporting of data. Yet with seemingly more job roles, demand, and resources to learn data science than ever (Davenport & Patil, 2022), many of those attempting to break into the field find themselves frustrated and disillusioned (Selvaraj, 2022).\nThis mismatch between the expectations and experiences of data science hopefuls underscores the need to characterize the skills and experiences necessary to succeed in a data science role. Greater transparency surrounding these factors would demystify the field as a whole and provide a better road map for employment-ready data science professionals. Using an open dataset on data science jobs from Kaggle (Zangari, n.d.), this study sought to discern whether there was a categorical association between experience level and job category in data science jobs. Following a statistically significant Chi-Square Test of Independence omnibus test, the experience level composition of individual job categories would be compared using the same Chi-Square test. An additional Random Forest Regression (RFR) model was used to estimate salary from experience level, job category, and work setting.\n\n1.1: Research Question and Hypotheses\nDoes experience level composition vary between data science job categories?\nNull Hypothesis (H0): The proportion of employees at each experience level does not vary between job categories in the dataset.\nAlternative Hypothesis (HA): The proportion of employees at each experience level varies between job categories in the dataset."
  },
  {
    "objectID": "posts/capstone_report/code/capstone_report.html#data-collection",
    "href": "posts/capstone_report/code/capstone_report.html#data-collection",
    "title": "Chi-Square and Random Forest Regression with Data Science Jobs Dataset",
    "section": "2: Data Collection",
    "text": "2: Data Collection\nThis data used in this study is publicly available on Kaggle (Zangari, n.d.). The data was originally collected and compiled by ai-jobs.net (2024), and features responses from data science professionals and employers. The main adaptation made in the Kaggle version was the addition of the job_categories feature, which collapsed 149 unique job titles into ten general categories. This was highly advantageous for the scope of this study, as it would be difficult to compare and interpret differences between 149 different job titles.\n\n# import useful libraries\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import Markdown, display\n\n\n# read in csv file as dataframe\njob_df = pd.read_csv('../data/jobs_in_data_2024.csv')\n\n\n2.1: Exploratory Data Analysis\nThe dataset contained 14,199 records without any missing values for any columns. The chronological range of responses began in 2020, with some responses as recent as this year (i.e., 2024). Since some features pertained to similar constructs (e.g., job_title and job_category), it was important to gauge the usability of features and levels therein. For instance, some features had a large number of levels (e.g., job_title, employee_residence, and company_location), which can be difficult to analyze. Additionally, features like employment_type, employee_residence, and company_location exhibited considerable categorical imbalances, with the vast majority of responses belonging to one category.\n\n# get initial info for df\njob_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14199 entries, 0 to 14198\nData columns (total 12 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   work_year           14199 non-null  int64 \n 1   experience_level    14199 non-null  object\n 2   employment_type     14199 non-null  object\n 3   job_title           14199 non-null  object\n 4   salary              14199 non-null  int64 \n 5   salary_currency     14199 non-null  object\n 6   salary_in_usd       14199 non-null  int64 \n 7   employee_residence  14199 non-null  object\n 8   work_setting        14199 non-null  object\n 9   company_location    14199 non-null  object\n 10  company_size        14199 non-null  object\n 11  job_category        14199 non-null  object\ndtypes: int64(3), object(9)\nmemory usage: 1.3+ MB\n\n\n\n# get number of unique values for each variable \nfor column in job_df.columns:\n    print(column, len(job_df[column].value_counts()))\n\nwork_year 5\nexperience_level 4\nemployment_type 4\njob_title 149\nsalary 2229\nsalary_currency 12\nsalary_in_usd 2578\nemployee_residence 86\nwork_setting 3\ncompany_location 74\ncompany_size 3\njob_category 10\n\n\n\n# get value counts for each object variable\nfor column in job_df.columns:\n    if job_df[column].dtype == object:\n        print(job_df[column].value_counts(), \"\\n\")\n\nexperience_level\nSenior         9381\nMid-level      3339\nEntry-level    1063\nExecutive       416\nName: count, dtype: int64 \n\nemployment_type\nFull-time    14139\nContract        26\nPart-time       22\nFreelance       12\nName: count, dtype: int64 \n\njob_title\nData Engineer                    3059\nData Scientist                   2910\nData Analyst                     2120\nMachine Learning Engineer        1488\nResearch Scientist                454\n                                 ... \nData Analytics Associate            1\nAnalytics Engineering Manager       1\nSales Data Analyst                  1\nAWS Data Architect                  1\nConsultant Data Engineer            1\nName: count, Length: 149, dtype: int64 \n\nsalary_currency\nUSD    13146\nGBP      538\nEUR      422\nCAD       51\nAUD       12\nPLN        7\nCHF        6\nSGD        6\nBRL        4\nTRY        3\nDKK        3\nNZD        1\nName: count, dtype: int64 \n\nemployee_residence\nUnited States     12418\nUnited Kingdom      616\nCanada              371\nSpain               131\nGermany              90\n                  ...  \nAndorra               1\nUganda                1\nOman                  1\nQatar                 1\nLuxembourg            1\nName: count, Length: 86, dtype: int64 \n\nwork_setting\nIn-person    9413\nRemote       4573\nHybrid        213\nName: count, dtype: int64 \n\ncompany_location\nUnited States           12465\nUnited Kingdom            623\nCanada                    373\nSpain                     127\nGermany                    96\n                        ...  \nAndorra                     1\nQatar                       1\nMauritius                   1\nGibraltar                   1\nMoldova, Republic of        1\nName: count, Length: 74, dtype: int64 \n\ncompany_size\nM    13112\nL      919\nS      168\nName: count, dtype: int64 \n\njob_category\nData Science and Research         4675\nData Engineering                  3157\nData Analysis                     2204\nMachine Learning and AI           2148\nLeadership and Management          791\nBI and Visualization               600\nData Architecture and Modeling     419\nData Management and Strategy       115\nData Quality and Operations         79\nCloud and Database                  11\nName: count, dtype: int64 \n\n\n\n# show distribution for salary_in_usd\nsns.histplot(job_df['salary_in_usd'])\nplt.show()\n\n# show distribution for salary\nsns.histplot(job_df['salary'])\nplt.show()"
  },
  {
    "objectID": "posts/capstone_report/code/capstone_report.html#data-extraction-and-preparation",
    "href": "posts/capstone_report/code/capstone_report.html#data-extraction-and-preparation",
    "title": "Chi-Square and Random Forest Regression with Data Science Jobs Dataset",
    "section": "3: Data Extraction and Preparation",
    "text": "3: Data Extraction and Preparation\nDue to categorical imbalances shown in the previous section, the dataset was filtered to only include employees residing in the U.S. (employee_residence), working for a U.S.-based company (company_location), and working full-time (employment_type). While this reduced the potential generalizability of findings, it was important to recognize that the data was already biased towards these groups, and any attempt to generalize beyond that would be undermined to begin with. Filtering for these categories reduced the number of observations to 12,389, which constituted less than 13% data loss. Features work_setting and company_size were not filtered to remedy categorical imbalances, as each feature contained only three levels, and the degree of the imbalance was less concerning. Potential outliers and skewness in salary_in_usd were not treated, as Random Forest models are robust to outliers and extreme values (Deori, 2023), and high salaries were potentially noteworthy in the RFR model.\nOf the twelve original features, only four were kept for analyses: experience_level, salary_in_usd, work_setting, and job_category. After filtering for employment_type, employee_residence, company_location, these three features were excluded on the basis that each was no longer a variable (i.e., each only had one value). The feature job_category was used place of job_title, as the former had far lower cardinality (i.e., ten unique value vs. 149), making it far easier to incorporate in analyses. Filtering by country meant salary_in_usd could be used in place of salary and salary_currency, as conversions would not be necessary. Features work_year and company_size would have been nice to add to the salary estimation Random Forest Regression model, but both had sub-optimal distributions and level structures to analyze.\n\n# create separate df for just US employees working for US companies\nus_job_df = job_df.loc[(job_df['employee_residence'] == 'United States') & \n                       (job_df['company_location'] == 'United States')]\n\n\n# filter df to only include full-time employees\nus_job_df = us_job_df.loc[us_job_df['employment_type'] == 'Full-time']\nprint(us_job_df['employment_type'].value_counts())\n\nemployment_type\nFull-time    12389\nName: count, dtype: int64\n\n\n\n# reset index to align with observations\nus_job_df.reset_index(inplace=True, drop=True)\n\n\n# keep only relevant variables\nrefined_us_job_df = us_job_df[['experience_level', 'salary_in_usd', 'work_setting',\n                               'job_category']]\n\n\n3.A: Chi-Square Model Preparation\nInitial cross tabulations characterized the expected frequencies of experience_level and job_category before running the Chi-Square model (see Table 1). The a-priori criteria defined in this project’s proposal was that any job categories with fewer than five expected counts in any cell would be combined with other job categories meeting this criteria. Such low expected counts would be abnormally low for a dataset of this size, and collapsing categories can serve as a sound solution without incurring data loss (Bewick et al., 2003). The job categories Cloud and Database, Data Management and Strategy, and Data Quality and Operations all had fewer than five expected counts for at least one experience level, leading these to be combined into one category (see Table 1 (a)). Rather than labeling this category as “Other,” these categories all pertained to Data Management, which served as a more descriptive label (see Table 1 (b)). The final cross tabulation to be used in the Chi-Square analysis was saved as trans_refined_ct.\n\n# create crosstab for experience_level and job_category, with margins\nimport statsmodels.api as sm\n#pd.set_option('display.float_format', lambda x: '{0:.2f}' % x)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.precision', 3)\n\n#pd.set_option('display.precision', 2)\ncross_tab = pd.crosstab(refined_us_job_df['experience_level'], \n                        refined_us_job_df['job_category'], \n                        margins=True)\n\n\n# obtain expected values for crosstab\nexpected_table = sm.stats.Table(cross_tab)\n#expected_table.fittedvalues.round(2).T\n\n\n# create copy before transforming levels\nct_us_job_df = refined_us_job_df.copy(deep=True)\n\n\n# convert 3 levels to Data Management\nct_us_job_df.replace({'job_category': \n                      {'Cloud and Database': 'Data Management', \n                       'Data Management and Strategy': 'Data Management', \n                       'Data Quality and Operations': 'Data Management'}}, \n                      inplace=True)\n\n\n# define and output new contingency table\nrefined_ct = pd.crosstab(ct_us_job_df['experience_level'], \n                         ct_us_job_df['job_category'])\nrefined_expected_table = sm.stats.Table(refined_ct)\n\n\n# create expected table with margins\nmarginal_ct = pd.crosstab(ct_us_job_df['experience_level'], \n                          ct_us_job_df['job_category'], margins=True)\nmarginal_expected_table = sm.stats.Table(marginal_ct)\n\n\n# create rounded, transposed versions of tables\nct_exp_md = expected_table.fittedvalues.round(2).T\nct_ref_exp_md = marginal_expected_table.fittedvalues.round(2).T\n\n# display markdown versions of tables\ndisplay(Markdown(ct_exp_md.to_markdown()))\ndisplay(Markdown(ct_ref_exp_md.to_markdown()))\n\n\n\nTable 1: Original and refined expected counts contingency tables.\n\n\n\n\n\n\n\n(a) Original (ten job categories)\n\n\n\n\n\n\n\n\n\n\n\n\n\njob_category\nEntry-level\nExecutive\nMid-level\nSenior\nAll\n\n\n\n\nBI and Visualization\n33.39\n15.52\n115.16\n365.96\n529.97\n\n\nCloud and Database\n0.72\n0.34\n2.5\n7.94\n11.5\n\n\nData Analysis\n119.19\n55.39\n411.09\n1306.42\n1891.9\n\n\nData Architecture and Modeling\n23.77\n11.05\n81.97\n260.49\n377.23\n\n\nData Engineering\n175.44\n81.54\n605.12\n1923.03\n2784.86\n\n\nData Management and Strategy\n6.13\n2.85\n21.13\n67.15\n97.25\n\n\nData Quality and Operations\n4.17\n1.94\n14.39\n45.75\n66.25\n\n\nData Science and Research\n260.55\n121.1\n898.66\n2855.9\n4135.79\n\n\nLeadership and Management\n43.15\n20.06\n148.84\n472.99\n684.97\n\n\nMachine Learning and AI\n114.02\n52.99\n393.27\n1249.8\n1809.91\n\n\nAll\n780.46\n362.73\n2691.86\n8554.57\n12388.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Refined (eight job categories)\n\n\n\n\n\n\n\n\n\n\n\n\n\njob_category\nEntry-level\nExecutive\nMid-level\nSenior\nAll\n\n\n\n\nBI and Visualization\n33.38\n15.5\n115.16\n365.97\n529.99\n\n\nData Analysis\n119.15\n55.32\n411.1\n1306.46\n1891.96\n\n\nData Architecture and Modeling\n23.76\n11.03\n81.97\n260.5\n377.24\n\n\nData Engineering\n175.39\n81.43\n605.14\n1923.09\n2784.94\n\n\nData Management\n10.97\n5.09\n37.86\n120.32\n174.25\n\n\nData Science and Research\n260.48\n120.93\n898.69\n2855.98\n4135.92\n\n\nLeadership and Management\n43.14\n20.03\n148.84\n473\n684.99\n\n\nMachine Learning and AI\n113.99\n52.92\n393.29\n1249.84\n1809.96\n\n\nAll\n780.23\n362.24\n2691.95\n8554.83\n12388.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n# save final contingency table for use in analyses\ntrans_refined_ct = refined_ct.T\n\n\n\n3.B: Random Forest Regressor Model Preparation\n\n3.B.1: Encoding Section\nSince scikit-learn’s RFR implementation can only accept numeric features, predictors for the RFR model (experience_level, work_setting, and job_category) were transformed using a one-hot encoding (OHE) method. While added sparsity from OHE features can reduce model efficiency and performance (Ravi, 2022), other encoding methods that do not increase sparsity (e.g., integer or ordinal encoding) can create unintended relationships between feature levels (Brownlee, 2020b). Such ordered relationships would have been defensible for features experience_level and work_setting, but these were also encoded using an OHE method for consistency with the job_category feature. By using the ColumnTransformer() function to apply OHE, the target variable, salary_in_usd, was left unaffected, and the output was automatically transformed into a pandas dataframe. This resulted in seventeen OHE features, corresponding to the levels of experience_level (n = 4), work_setting (n = 3), and job_category (n = 10).\n\n# import libraries necessary for transformation\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# get list of categorical columns to transform\ncat_df = refined_us_job_df[['experience_level', 'work_setting', 'job_category']]\ncat_cols = list(cat_df.keys())\nprint(cat_cols)\n\n['experience_level', 'work_setting', 'job_category']\n\n\n\n# define transformer with non-sparse output\ncat_transformer = OneHotEncoder(sparse_output=False)\n\n\n# apply transformer to cat_cols, while passing over salary_in_usd\nct = ColumnTransformer([('cat', cat_transformer, cat_cols)], remainder='passthrough')\n\n\n# set output to pandas df and fit to data\nct.set_output(transform='pandas')\ndc_us_job_df = ct.fit_transform(refined_us_job_df)\ndc_us_job_df.columns\n\nIndex(['cat__experience_level_Entry-level', 'cat__experience_level_Executive',\n       'cat__experience_level_Mid-level', 'cat__experience_level_Senior',\n       'cat__work_setting_Hybrid', 'cat__work_setting_In-person',\n       'cat__work_setting_Remote', 'cat__job_category_BI and Visualization',\n       'cat__job_category_Cloud and Database',\n       'cat__job_category_Data Analysis',\n       'cat__job_category_Data Architecture and Modeling',\n       'cat__job_category_Data Engineering',\n       'cat__job_category_Data Management and Strategy',\n       'cat__job_category_Data Quality and Operations',\n       'cat__job_category_Data Science and Research',\n       'cat__job_category_Leadership and Management',\n       'cat__job_category_Machine Learning and AI',\n       'remainder__salary_in_usd'],\n      dtype='object')\n\n\n\n\n3.B.2: Feature Selection\nOnce the features were all in a numeric format, scikit-learn’s SelectKBest() discerned which variables were significantly associated with salary (see Table 2 for a summary). The scoring function used was f_regression(), due to the target feature being continuous. Three features were not significantly associated with salary_in_usd at the p &lt; .05 level (cat__work_setting_Hybrid, cat__job_category_Data Architecture and Modeling, cat__job_category_Cloud and Database). These features were dropped from subsequent analyses, leaving a total of fourteen features.\n\n# import libraries for feature selection\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n\n# create X and y arrays for feature selection and beyond\ny = dc_us_job_df['remainder__salary_in_usd']\nX = dc_us_job_df.drop(['remainder__salary_in_usd'], axis=1)\nprint(f'''X shape: {X.shape}, y shape: {y.shape}''')\n\nX shape: (12389, 17), y shape: (12389,)\n\n\n\n# create SelectKBest instance using f_regression on all features\nfeature_rank = SelectKBest(score_func=f_regression, k='all')\n# fit to data\nfeature_rank.fit(X, y=y)\n\nSelectKBest(k='all', score_func=&lt;function f_regression at 0x15b8caf20&gt;)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SelectKBestSelectKBest(k='all', score_func=&lt;function f_regression at 0x15b8caf20&gt;)\n\n\n\n# create df to show features, F-scores, and p-values\nfeature_df = pd.DataFrame({'Feature': X.columns, \n                           'F-score': feature_rank.scores_.round(2), \n                           'P-value': feature_rank.pvalues_.round(3)})\n# sort lowest p-values first \nfeature_df = feature_df.sort_values(by='P-value', ascending=True)\ndisplay(Markdown(feature_df.to_markdown()))\n\n\n\nTable 2: Feature importances ranked by significance.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nF-score\nP-value\n\n\n\n\n0\ncat__experience_level_Entry-level\n643.87\n0\n\n\n14\ncat__job_category_Data Science and Research\n321.51\n0\n\n\n13\ncat__job_category_Data Quality and Operations\n53.27\n0\n\n\n12\ncat__job_category_Data Management and Strategy\n97.05\n0\n\n\n9\ncat__job_category_Data Analysis\n1323\n0\n\n\n15\ncat__job_category_Leadership and Management\n23.38\n0\n\n\n7\ncat__job_category_BI and Visualization\n89.74\n0\n\n\n16\ncat__job_category_Machine Learning and AI\n791.71\n0\n\n\n5\ncat__work_setting_In-person\n23.33\n0\n\n\n3\ncat__experience_level_Senior\n838.27\n0\n\n\n2\ncat__experience_level_Mid-level\n509.32\n0\n\n\n1\ncat__experience_level_Executive\n169.43\n0\n\n\n6\ncat__work_setting_Remote\n21.73\n0\n\n\n11\ncat__job_category_Data Engineering\n10.23\n0.001\n\n\n4\ncat__work_setting_Hybrid\n2.18\n0.14\n\n\n10\ncat__job_category_Data Architecture and Modeling\n1.64\n0.2\n\n\n8\ncat__job_category_Cloud and Database\n0.58\n0.446\n\n\n\n\n\n\n\n\n\n# drop non-significant p-values from features\nX = X.drop(['cat__work_setting_Hybrid', \n            'cat__job_category_Data Architecture and Modeling', \n            'cat__job_category_Cloud and Database'], axis=1)\n\n\n\n3.B.3: Train-Test-Split\nFollowing the conventions of machine learning best practices, the data for the RFR model was split into training and testing sets, with a ratio of 70:30 in favor of the training set. This ensures the RFR model can generalize beyond the data it trained on (i.e., to the testing set), since this resembles the type of predictive modeling paradigm wherein the predicted values are not available (Brownlee, 2020a). This 70:30 split resulted in training sets with 8,672 observations and testing sets with 3,717 observations.\n\n# create train and test arrays from X and y with 80% train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, \n                                                    random_state=21)\n\n\n# output dimensions of train and test sets \nprint(f'''X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, \n      y_train shape: {y_train.shape}, y_test shape: {y_test.shape}''')\n\nX_train shape: (8672, 14), X_test shape: (3717, 14), \n      y_train shape: (8672,), y_test shape: (3717,)"
  },
  {
    "objectID": "posts/capstone_report/code/capstone_report.html#analysis",
    "href": "posts/capstone_report/code/capstone_report.html#analysis",
    "title": "Chi-Square and Random Forest Regression with Data Science Jobs Dataset",
    "section": "4: Analysis",
    "text": "4: Analysis\n\n4.A: Chi-Square Model\n\n4.A.1: Omnibus\nThe omnibus-level Chi-Square Test of Independence was statistically significant (X2 = 1003.54, p &lt; .001). Thus, the null hypothesis was rejected, as the experience level composition of employees varied between job categories. This necessitated further post hoc testing to compare experience level composition between job categories.\n\n# output omnibus Chi-Square result\nfrom scipy.stats import chi2_contingency\nresult = chi2_contingency(trans_refined_ct)\nprint(f'''Statistic: {result.statistic}, P-value: {result.pvalue}''')\n\nStatistic: 1003.5413047409224, P-value: 4.926315041672239e-199\n\n\n\n\n4.A.2: Post Hoc Tests\nThe post hoc tests were completed using code adapted from Neuhof (2018), as this helped streamline the process for the 28 job category comparisons. Instead of printing these comparisons as in the original code, converting to and printing a dataframe yielded a cleaner output (see Table 3). Given the large dataset at hand, a more stringent Bonferroni correction was used to correct for alpha inflation (Jafari & Ansari-Pour, 2019), as this method multiples the statistical significance by the number of comparisons made. Despite this, all 28 comparisons were statistically significant after the Bonferroni correction (Bonferroni-corrected p-range: p &lt; .001 - p = .001). Focusing instead on the magnitude of the effect size in terms of X2, the three strongest effects resulted from comparisons with Data Analysis (compared to Data Engineer, Machine Learning and AI, and Data Science and Research, respectively). The visualizations in the following suggest that Data Analysis had more Entry-level employees relative to the comparison categories.\n\n# code adapted from Neuhof (2018)\nfrom itertools import combinations\n\n# gathering all combinations for post-hoc chi2\nall_combinations = list(combinations(trans_refined_ct.index, 2))\nchi2_vals = []\np_vals = []\nfor comb in all_combinations:\n    # subset df into a dataframe containing only the pair \"comb\"\n    new_df = trans_refined_ct[(trans_refined_ct.index == comb[0]) | \n                              (trans_refined_ct.index == comb[1])]\n    # running chi2 test\n    chi2, p, dof, ex = chi2_contingency(new_df, correction=True)\n    chi2_vals.append(chi2)\n    p_vals.append(p)\n\n\n# create dataframe with combinations, effect sizes, and p-vals\nchi2_df = pd.DataFrame({'Combination': all_combinations, \n                        'Statistic': chi2_vals, \n                        'p': p_vals})\n# create Bonferroni-corrected p-values by multiplying by number of tests\nchi2_df['p-Bonf'] = (chi2_df['p'] * 28).round(3)\nchi2_df['Statistic'] = chi2_df['Statistic'].round(2)\nchi2_df['p'] = chi2_df['p'].round(3)\n# sort df by effect size \nchi2_df = chi2_df[['Combination', 'Statistic', 'p', 'p-Bonf']].sort_values(\nby='Statistic', ascending=False)\n# print as markdown\ndisplay(Markdown(chi2_df.to_markdown()))\n\n\n\nTable 3: Job category comparisons ranked by Chi-Square statistic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombination\nStatistic\np\np-Bonf\n\n\n\n\n8\n(‘Data Analysis’, ‘Data Engineering’)\n340.85\n0\n0\n\n\n12\n(‘Data Analysis’, ‘Machine Learning and AI’)\n325.68\n0\n0\n\n\n10\n(‘Data Analysis’, ‘Data Science and Research’)\n297.49\n0\n0\n\n\n11\n(‘Data Analysis’, ‘Leadership and Management’)\n199.27\n0\n0\n\n\n27\n(‘Leadership and Management’, ‘Machine Learning and AI’)\n170.07\n0\n0\n\n\n7\n(‘Data Analysis’, ‘Data Architecture and Modeling’)\n156.55\n0\n0\n\n\n14\n(‘Data Architecture and Modeling’, ‘Data Management’)\n149.71\n0\n0\n\n\n24\n(‘Data Management’, ‘Machine Learning and AI’)\n140.46\n0\n0\n\n\n25\n(‘Data Science and Research’, ‘Leadership and Management’)\n127.48\n0\n0\n\n\n16\n(‘Data Architecture and Modeling’, ‘Leadership and Management’)\n122.45\n0\n0\n\n\n1\n(‘BI and Visualization’, ‘Data Architecture and Modeling’)\n96.26\n0\n0\n\n\n18\n(‘Data Engineering’, ‘Data Management’)\n86.38\n0\n0\n\n\n6\n(‘BI and Visualization’, ‘Machine Learning and AI’)\n84.62\n0\n0\n\n\n21\n(‘Data Engineering’, ‘Machine Learning and AI’)\n75.22\n0\n0\n\n\n22\n(‘Data Management’, ‘Data Science and Research’)\n73.34\n0\n0\n\n\n13\n(‘Data Architecture and Modeling’, ‘Data Engineering’)\n70.68\n0\n0\n\n\n15\n(‘Data Architecture and Modeling’, ‘Data Science and Research’)\n63.27\n0\n0\n\n\n23\n(‘Data Management’, ‘Leadership and Management’)\n57.52\n0\n0\n\n\n26\n(‘Data Science and Research’, ‘Machine Learning and AI’)\n56.39\n0\n0\n\n\n0\n(‘BI and Visualization’, ‘Data Analysis’)\n53\n0\n0\n\n\n20\n(‘Data Engineering’, ‘Leadership and Management’)\n50.03\n0\n0\n\n\n19\n(‘Data Engineering’, ‘Data Science and Research’)\n39.92\n0\n0\n\n\n9\n(‘Data Analysis’, ‘Data Management’)\n30.91\n0\n0\n\n\n5\n(‘BI and Visualization’, ‘Leadership and Management’)\n29.99\n0\n0\n\n\n2\n(‘BI and Visualization’, ‘Data Engineering’)\n27\n0\n0\n\n\n3\n(‘BI and Visualization’, ‘Data Management’)\n25.48\n0\n0\n\n\n4\n(‘BI and Visualization’, ‘Data Science and Research’)\n22.98\n0\n0.001\n\n\n17\n(‘Data Architecture and Modeling’, ‘Machine Learning and AI’)\n22.92\n0\n0.001\n\n\n\n\n\n\n\n\n\n\n4.A.3: Chi-Square Visualizations\nThe contingency cross tabulation tables are shown below for the observed (see Table 4 (a)) and expected counts (see Table 4 (b)). Rather than focusing on these individually, a residual table was created to show observed counts deviated from the expected counts for each cell (see Table 4 (c)). Residuals were calculated as the difference between the observed and expected counts divided by the expected values ((O - E) / E), which yields both the direction and magnitude of difference between the two. The accompanying heatmap (see Figure 1) shows where counts were greater than expected in red (e.g., Entry-level and Data Analysis or Executive and Leadership and Management) and fewer than expected in blue (e.g., Entry-level and Data Architecture and Modeling or Executive and Data Management). The bivariate plots compare the experience level composition across job categories, using the proportion (Figure 2 (a)) and count of employees (Figure 2 (b)) with each experience level. Figure 2 (a) re-iterates the findings that Data Analysis for Entry-level employees and Leadership and Management for Executive employees were relative peaks among less-represented experience levels. Conversely, Senior employees comprised a sizable portion of every job category, and the clear majority of a few (e.g., Data Architecture and Modeling, Machine Learning and AI). Figure 2 (b) uses number of employees for the y-axis variable, which takes into account size differences between job categories. This again depicts the relative commonality of Senior employees, as three of four largest categories (Data Science and Research, Data Engineering, and Machine Learning and AI) contained a substantial number of Senior employees.\n\n# set order of Chi-Square variables\n# set experience order\nexperience_order = pd.CategoricalDtype(['Entry-level', 'Mid-level', 'Senior', \n                                        'Executive'], ordered=True)\nct_us_job_df.experience_level = ct_us_job_df.experience_level.astype(experience_order)\n# set job category order to alphabetical\ncategory_order = pd.CategoricalDtype(['BI and Visualization', 'Data Analysis', \n                                      'Data Architecture and Modeling', \n                                      'Data Engineering', 'Data Management', \n                                      'Data Science and Research', \n                                      'Leadership and Management', \n                                      'Machine Learning and AI'], ordered=True)\nct_us_job_df.job_category = ct_us_job_df.job_category.astype(category_order)\n\n\n# create crosstabs without margins\nobserved_ct = pd.crosstab(ct_us_job_df['experience_level'],\n                          ct_us_job_df['job_category'], margins=False)\nexpected_ct = sm.stats.Table(observed_ct)\n\n\n# convert cts to dfs and transpose\nexpected_df = pd.DataFrame(expected_ct.fittedvalues).T\nobserved_df = pd.DataFrame(observed_ct).T\n\n\n# run transformations to create residuals \nresid_div_ct_df = observed_df.subtract(expected_df)\nresid_div_ct_df = resid_div_ct_df.divide(expected_df)\n\n\n# round all dfs for better outputting\nexpected_df = expected_df.round(2)\nobserved_df = observed_df.round(2)\nresid_div_ct_df = resid_div_ct_df.round(2)\n\n\n# display tables in markdown format\ndisplay(Markdown(observed_df.to_markdown()))\ndisplay(Markdown(expected_df.to_markdown(index=False)))\ndisplay(Markdown(resid_div_ct_df.to_markdown()))\n\n\n\nTable 4: Contingency Tables\n\n\n\n\n\n\n\n(a) Observed counts.\n\n\n\n\n\n\n\n\n\n\n\n\njob_category\nEntry-level\nMid-level\nSenior\nExecutive\n\n\n\n\nBI and Visualization\n37\n150\n325\n18\n\n\nData Analysis\n347\n450\n1075\n20\n\n\nData Architecture and Modeling\n0\n34\n339\n4\n\n\nData Engineering\n95\n623\n1937\n130\n\n\nData Management\n22\n74\n78\n0\n\n\nData Science and Research\n216\n866\n2954\n100\n\n\nLeadership and Management\n16\n216\n393\n60\n\n\nMachine Learning and AI\n47\n279\n1454\n30\n\n\n\n\n\n\n\n\n\n\n\n(b) Expected counts.\n\n\n\n\n\nEntry-level\nMid-level\nSenior\nExecutive\n\n\n\n\n33.39\n115.15\n365.95\n15.51\n\n\n119.19\n411.08\n1306.38\n55.36\n\n\n23.78\n82.02\n260.65\n11.04\n\n\n175.44\n605.1\n1922.98\n81.48\n\n\n10.99\n37.91\n120.49\n5.11\n\n\n260.54\n898.64\n2855.81\n121.01\n\n\n43.15\n148.83\n472.98\n20.04\n\n\n114.02\n393.26\n1249.76\n52.96\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Residual counts.\n\n\n\n\n\n\n\n\n\n\n\n\njob_category\nEntry-level\nMid-level\nSenior\nExecutive\n\n\n\n\nBI and Visualization\n0.11\n0.3\n-0.11\n0.16\n\n\nData Analysis\n1.91\n0.09\n-0.18\n-0.64\n\n\nData Architecture and Modeling\n-1\n-0.59\n0.3\n-0.64\n\n\nData Engineering\n-0.46\n0.03\n0.01\n0.6\n\n\nData Management\n1\n0.95\n-0.35\n-1\n\n\nData Science and Research\n-0.17\n-0.04\n0.03\n-0.17\n\n\nLeadership and Management\n-0.63\n0.45\n-0.17\n1.99\n\n\nMachine Learning and AI\n-0.59\n-0.29\n0.16\n-0.43\n\n\n\n\n\n\n\n\n\n\n\n\n\n# show residuals using heatmap using blue and red colors, centering on 0\nresid_heatmap = sns.heatmap(resid_div_ct_df, annot=True, cmap='coolwarm', center=0)\nresid_heatmap.set(xlabel='Experience Level', ylabel='Job Category')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Chi-Square residual heatmap. Positive values denote greater observed counts than expected in red. Negative values denoting fewer observed counts than expected are shown in blue.\n\n\n\n\n\n\n# reset experience_order to show experience vertically\nexperience_order = pd.CategoricalDtype(['Executive', 'Senior', 'Mid-level', \n                                        'Entry-level'], ordered=True)\nct_us_job_df.experience_level = ct_us_job_df.experience_level.astype(experience_order)\n\n# show bivariate plots for experience level and job category proportions and counts\nwith sns.color_palette('colorblind'):\n    prop_hist = sns.histplot(data=ct_us_job_df, x='job_category', \n                             hue='experience_level', stat='proportion', \n                             multiple='fill', discrete=True)\n    prop_hist.set_xticklabels(['BI/Viz', 'DA', 'DArc/Modeling', 'DE', 'DM', \n                               'DS/Research', 'Leadership', 'ML/AI'], rotation=45)\n    prop_hist.set(xlabel='Job Category', ylabel='Proportion of Employees')#, \n                  #title='Experience Level Composition by Job Category')\n    prop_hist.get_legend().set_title('Experience Level')\n    plt.show()\n\n    count_hist = sns.histplot(data=ct_us_job_df, x='job_category', \n                              hue='experience_level', multiple='stack')\n    count_hist.set_xticklabels(['BI/Viz', 'DA', 'DArc/Modeling', 'DE', 'DM', \n                                'DS/Research', 'Leadership', 'ML/AI'], rotation=45)\n    count_hist.set(xlabel='Job Category', ylabel='Number of Employees')\n    count_hist.get_legend().set_title('Experience Level')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of job category with each experience level.\n\n\n\n\n\n\n\n\n\n\n\n(b) Number of employees with experience level across job categories.\n\n\n\n\n\n\n\nFigure 2: Experience level and job category composition. Abbreviations: BI/Viz: BI and Visualization; DA: Data Analysis; DArc/Modeling: Data Architecture and Modeling; DE: Data Engineering; DM: Data Management; DS/Research: Data Science and Research; Leadership: Leadership and Management; ML/AI: Machine Learning and AI.\n\n\n\n\n\n\n4.B: Random Forest Regressor Model\n\n4.B.1: Grid Search for RFR\nTo create a more robust RFR model, hyperparameters of interest were tuned using GridSearchCV to evaluate hyperparameter combinations with the training data. The parameter grid was constructed using guidelines from Ellis (2022), specifying values in a reasonable range for n_estimators, max_features, and one parameter to limit tree depth (e.g., max_depth). With root-mean-squared error (RMSE) as the a-priori error metric for the RFR model, the scoring metric for the grid search object was set to neg_root_mean_squared_error. This grid search object was supplied a default RFR model and the parameter grid, and subsequently fit to the X_train and y_train datasets for five cross-validation folds. The resulting best parameters were max_depth = 8, max_features = ‘sqrt’, and n_estimators = 50, which were used with the models in the next section (see section 4.B.2: Train and Test Model Performance).\n\n# import libraries necessary for hyperparameter tuning\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# define MSE for later use\nMSE = metrics.mean_squared_error\n\n\n# instantiate baseline model\nrf_model = RandomForestRegressor(random_state=21)\n\n\n# construct parameter grid to for hyperparameters of interest\nparam_grid = {'n_estimators': [50, 100, 200, 400], \n              'max_depth': [2, 4, 8, None],\n              'max_features': ['sqrt', 'log2', None]\n              }\n\n\n# fit GridSearchCV to rf_model with param_grid and scoring as negative RMSE\ngs_rf = GridSearchCV(rf_model, param_grid=param_grid, \n                     scoring='neg_root_mean_squared_error', cv=5)\n\n\n# fit grid search object to data\ngs_rf.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=21),\n             param_grid={'max_depth': [2, 4, 8, None],\n                         'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [50, 100, 200, 400]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=21),\n             param_grid={'max_depth': [2, 4, 8, None],\n                         'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [50, 100, 200, 400]},\n             scoring='neg_root_mean_squared_error')estimator: RandomForestRegressorRandomForestRegressor(random_state=21)RandomForestRegressorRandomForestRegressor(random_state=21)\n\n\n\n# output best parameters from grid search\nprint(gs_rf.best_params_)\n\n{'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 50}\n\n\n\n\n4.B.2: Train and Test Model Performance\nThe training model performance was evaluated from the grid search object directly, from which the RMSE and R2 values were extracted. The final RFR model was created with tuned hyperparameters and fit to X_train and y_train, and subsequently used to predict y_test using X_test. Model performance was comparable in the training (RMSE = 53,050.72, R2 = .2448) and testing models (RMSE = 54,352.70, R2 = .2525). The testing model performance indicates that the features explained roughly 25% of the variance in salary, and the salary estimates were off by roughly $54,000 on average. The convergence between the error and explained variance metrics between the training and testing models suggest the model was not overfit to the training data.\n\n# get RMSE and R2 from best grid search model (training)\ngrid_rmse = (gs_rf.best_score_ * -1)\nprint('Training RMSE: ', round(grid_rmse, 2))\ngrid_predict = gs_rf.predict(X_train)\nprint('Training R-Squared: ', round(metrics.r2_score(y_true=y_train, \n      y_pred=grid_predict), 4))\n\nTraining RMSE:  53050.72\nTraining R-Squared:  0.2448\n\n\n\n# define final RFR model with best parameters\nrfr_model = RandomForestRegressor(max_depth=8, \n                                  max_features='sqrt', \n                                  n_estimators=50, \n                                  random_state=21)\n\n\n# fit final model to X_train and y_train\nrfr_model.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=8, max_features='sqrt', n_estimators=50,\n                      random_state=21)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=8, max_features='sqrt', n_estimators=50,\n                      random_state=21)\n\n\n\n# using that RFR model, predict y_test using X_test\ny_test_pred = rfr_model.predict(X_test)\n\n\n# get RMSE and R2 values from test model\ntest_RMSE = np.sqrt(MSE(y_test, y_test_pred))\ntest_R2 = metrics.r2_score(y_test, y_test_pred)\nprint('Test RMSE: ', round(test_RMSE, 2))\nprint('Test R-Squared: ', round(test_R2, 4))\n\nTest RMSE:  54352.7\nTest R-Squared:  0.2525\n\n\n\n\n4.B.3: Feature Importances and Tree Diagram\nFeature importances were extracted from the RFR model, converted to a dataframe, and sorted in descending order (see Table 5). The values ranged from .008 to .286, with all features explaining at least some variance, and the magnitude gradually decreasing with each subsequent feature (see Figure 3). The top three features included the Data Analysis job category (.286), Machine Learning and AI job category (.162), and Senior experience level (.136). Unsurprisingly, these were the first three features used to split the data in the RFR tree diagram (see Figure 4), which underscores each feature’s importance to estimating salary. The code for this diagram was adapted from a TensorFlow tutorial (Visualizing TensorFlow Decision Forest Trees with Dtreeviz, 2024), and this figure makes it easy to interpret how the absence or presence of an experience level, job category, or work setting altered the RFR model’s salary estimate. With the depth of the model, however, only the first three levels of the diagram could be displayed before the clarity of the figure diminished.\n\n# create df for features and corresponding importances \nfeature_df = pd.DataFrame({'Feature': X.columns, \n                           'Importance': rfr_model.feature_importances_})\n# sort the values with higher importances first\nfeature_df['Importance'] = feature_df['Importance'].round(3)\nfeature_df = feature_df.sort_values(by='Importance', ascending=False)\n# display sorted df as markdown\ndisplay(Markdown(feature_df.to_markdown()))\n\n\n\nTable 5: Sorted feature importances in Random Forest model.\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n7\ncat__job_category_Data Analysis\n0.286\n\n\n13\ncat__job_category_Machine Learning and AI\n0.162\n\n\n3\ncat__experience_level_Senior\n0.136\n\n\n0\ncat__experience_level_Entry-level\n0.082\n\n\n2\ncat__experience_level_Mid-level\n0.082\n\n\n11\ncat__job_category_Data Science and Research\n0.079\n\n\n1\ncat__experience_level_Executive\n0.056\n\n\n6\ncat__job_category_BI and Visualization\n0.028\n\n\n8\ncat__job_category_Data Engineering\n0.022\n\n\n9\ncat__job_category_Data Management and Strategy\n0.02\n\n\n5\ncat__work_setting_Remote\n0.015\n\n\n4\ncat__work_setting_In-person\n0.013\n\n\n12\ncat__job_category_Leadership and Management\n0.012\n\n\n10\ncat__job_category_Data Quality and Operations\n0.008\n\n\n\n\n\n\n\n\n\n# create split version of encoded feature names\nfeature_df['sep_Variable'] = feature_df['Feature'].str.split(r'_', regex=True,\n                                                              expand=False)\n# take last portion of the split for cleaner feature names\nfeature_df['short_Variable'] = feature_df['sep_Variable'].str[-1]\n\n\n# visualize feature importances in descending order \nwith sns.color_palette('colorblind'):\n    feature_bar = sns.barplot(data=feature_df, x='Importance', y='short_Variable')\n    feature_bar.set(xlabel='Impurity-based Feature Importance', ylabel='Feature')\n    plt.show()\n\n\n\n\n\n\n\nFigure 3: Feature importances in Random Forest model.\n\n\n\n\n\n\n# code adapted from Visualizing TensorFlow Decision Trees with Dtreeviz (2024)\nimport dtreeviz\n\n# create dtreeviz model from RFR model\nviz_rfr_model = dtreeviz.model(rfr_model.estimators_[0], X, y, \n                               tree_index=3,\n                               feature_names=feature_df['short_Variable'], \n                               target_name='Salary')\n# only show first three levels of depth for sake of clarity\nviz_rfr_model.view(depth_range_to_display=[0,2], scale=1.2)\n\n\n\n\n\n\n\nFigure 4: Tree diagram from Random Forest model. Only the first three levels are shown to preserve figure clarity."
  },
  {
    "objectID": "posts/capstone_report/code/capstone_report.html#data-summary-and-implications",
    "href": "posts/capstone_report/code/capstone_report.html#data-summary-and-implications",
    "title": "Chi-Square and Random Forest Regression with Data Science Jobs Dataset",
    "section": "5: Data Summary and Implications",
    "text": "5: Data Summary and Implications\nThe results of the Chi-Square Test of Independence were statistically significant (see section 4.A.1: Omnibus), thus rejecting the null hypothesis that the experience level of employees did not vary between job categories (see section 1.1: Research Question and Hypotheses). With regards to this study’s research question, experience level composition varied between job categories, which suggests that some data science positions are typically held by more experienced employees than others. One limitation of using a Chi-Square test with this data was that it may have been overpowered, as all job category post hoc comparisons exhibited significant differences, even following a Bonferroni correction (see section 4.A.2: Post Hoc Tests). This shifted the focus from statistical significance of the comparisons to the magnitude of the effects, as quantified by the X2 values and the residuals, (see section 4.A.3: Chi-Square Visualizations for visualizations). Both metrics converged to suggest the Data Analysis job category was unique in its distribution of experience levels, which featured the highest proportion of entry-level employees.\nWhile the RFR model was not intended to test or evaluate hypotheses, the final model performed relatively well considering the challenges of the data (see section 4.B.2: Train and Test Model Performance). The features accounted for more than one-quarter of the variance in salary, which is impressive considering the fourteen encoded features originated from only three features. One limitation described in section 3.B.1: Encoding Section was the use of sparse categorical data in the RFR model, which has been shown to diminish performance or interpretability in some Random Forest models (Ravi, 2022). Interestingly, the RFR testing model performance (R2) outperformed the training model slightly (.2525 compared to .2448), which was unexpected. This can likely be attributed to the idiosyncratic, random nature of the train-test-split function (see section 3.B.3: Train-Test-Split), which may have given the testing set a slightly “easier” dataset to predict. Since the training model had a better RMSE value than the testing set (53,050.72 compared to 54,352.70), the training model still estimated salary better than the testing set, and the unexpected R2 comparison is likely not concerning.\nThe results of this study may be helpful for characterizing the field of data science at large. Firstly, the analyses using the data at hand suggest that entry-level positions are relatively rare in data science. This may lend credence to the idea that fewer entry-level positions are available in the field (Selvaraj, 2022), although, further studies should investigate this claim. The relative spike of entry-level employees in the data analysis job category supports multiple resources cited here (Selvaraj, 2022; Simmons, 2023). Each of these suggested a data analyst position may be an intermediate step for an entry-level employee to become a data scientist, and the results of this study suggest this may be plausible. Lastly, the results of the RFR model suggest some experience levels, job categories, and work settings tend to earn a higher salary than others. Notably, employees in the Data Analysis job category tended to earn less, whereas Senior employees, or employees in the Machine Learning and AI job category tended to earn more.\nFour considerations for future studies examining the field of data science are recommended following analyses here. One technical improvement would be to use an RFR model with non-sparse categorical features, to see if this potentially improves model performance or interpretability. An alternative to the scikit-learn workflow is the Distributed Random Forest model from H2O.ai (2024), which can incorporate nominal feature structures without requiring transformations. Concerning the association between experience level and job category, obtaining each employee’s entry-level position to data science would help create more holistic, trajectory-based snapshots of data science careers. This could also disentangle whether some roles with higher proportions of entry-level employees (e.g., Data Analysis) could feasibly serve as an intermediate step to another position (Selvaraj, 2022; Simmons, 2023). Concerning the RFR model, including more features that might be associated with data science salaries (e.g., education level, years of employment, or geographic location) could improve salary estimates and model performance. Lastly, acquiring more stratified data with respect to employee status, employee residence, and company location would help generalize analyses beyond full-time employees, residing in the U.S., and working for a U.S.-based company."
  },
  {
    "objectID": "posts/capstone_report/code/capstone_report.html#references",
    "href": "posts/capstone_report/code/capstone_report.html#references",
    "title": "Chi-Square and Random Forest Regression with Data Science Jobs Dataset",
    "section": "References",
    "text": "References\n\n\nai-jobs.net. (2024). Get a full dataset of global AI, ML, Data Salaries. https://ai-jobs.net/salaries/download/\n\n\nBewick, V., Cheek, L., & Ball, J. (2003). Statistics review 8: Qualitative data - tests of association. Critical Care, 8(1), 46. https://doi.org/10.1186/cc2428\n\n\nBrownlee, J. (2020a). Train-test split for evaluating machine learning algorithms. https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/\n\n\nBrownlee, J. (2020b). Why one-hot encode data in machine learning? https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n\n\nDavenport, T. H., & Patil, D. J. (2012). Data scientist: The sexiest job of the 21st century. Harvard Business Review. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century\n\n\nDavenport, T. H., & Patil, D. J. (2022). Is data scientist still the sexiest job of the 21st century? Harvard Business Review. https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century\n\n\nDeori, T. (2023). Demystifying Machine Learning Challenges: Outliers. https://levelup.gitconnected.com/demystifying-machine-learning-challenges-outliers-34aa4f45a1b9\n\n\nEllis, C. (2022). Hyperparameter tuning in random forests. https://crunchingthedata.com/hyperparameter-tuning-in-random-forests/\n\n\nH2O.ai. (2024). Distributed random forest (DRF)  H2O 3.46.0.1 documentation. https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html\n\n\nJafari, M., & Ansari-Pour, N. (2019). Why, when and how to adjust your p values? Cell Journal (Yakhteh), 20(4), 604–607. https://doi.org/10.22074/cellj.2019.5992\n\n\nNeuhof, M. (2018). Chi-square (and post-hoc) tests in Python. https://neuhofmo.github.io/chi-square-and-post-hoc-in-python/\n\n\nRavi, R. (2022). One-Hot Encoding is making your Tree-Based Ensembles worse, here’s why? https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769\n\n\nRosidi, N. (2023). Navigating data science job titles: Data analyst vs. Data scientist vs. Data engineer. https://www.kdnuggets.com/navigating-data-science-job-titles-data-analyst-vs-data-scientist-vs-data-engineer\n\n\nSelvaraj, N. (2022). Unable to land a data science job? Here’s why. https://www.kdnuggets.com/unable-to-land-a-data-science-job-heres-why\n\n\nSimmons, L. (2023). How to become a data scientist. https://www.computerscience.org/careers/data-science/how-to-become/\n\n\nU.S. Bureau of Labor Statistics. (2024). Data Scientists : Occupational Outlook Handbook: : U.S. Bureau of Labor Statistics. https://www.bls.gov/ooh/math/data-scientists.htm\n\n\nVisualizing TensorFlow Decision Forest Trees with dtreeviz. (2024). https://www.tensorflow.org/decision_forests/tutorials/dtreeviz_colab\n\n\nZangari, M. (n.d.). Jobs and Salaries in Data field 2024."
  },
  {
    "objectID": "posts/metal_subgenre_pairings/scraper.html",
    "href": "posts/metal_subgenre_pairings/scraper.html",
    "title": "Web Scraping Encyclopaedia Metallum",
    "section": "",
    "text": "Note: This is an adapted version of the original scraper.py script optimized for Quarto.\nAccordingly, the number of bands exported from this file to metallum_bands.db or metallum_bands.csv may differ slightly from those shown in subsequent analyses.\n\n# import necessary libraries\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver import ChromeOptions\nimport time\n\n\n# set options to fine-tune ChromeOptions()\noptions = ChromeOptions()\n# run headless ChromeDriver to avoid popup\noptions.add_argument('--headless=new')\n\n\n# instantiate Chrome driver with headless browser\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), \n                          options=options)\n# define base url \nbase_url = 'https://www.metal-archives.com/lists/'\n# instantiate driver from base_url\ndriver.get(base_url)\n\n\n# define function to extract band info from each individual page\ndef extract_band_info(page_source):\n    # create BeautifulSoup object to parse HTMl aspects of page\n    soup = BeautifulSoup(page_source, 'html.parser')\n    # locate band info rows and select\n    rows = soup.select('#bandListAlpha tbody tr')\n    \n    # create empty list to append data onto\n    band_info = []\n    \n    # iterate through rows (bands) in page\n    for row in rows:\n        # define entry as dictionary that gets only text aspects\n        # nth preserves increasing tr[] object\n        entry = {\n            'band_name': row.select_one('td:nth-of-type(1) a').text.strip(),\n            'country': row.select_one('td:nth-of-type(2)').text.strip(),\n            'genre': row.select_one('td:nth-of-type(3)').text.strip(),\n            'status': row.select_one('td:nth-of-type(4) span').text.strip()\n        }\n        # append band_info with each entry\n        band_info.append(entry)\n        # return the final band_info\n    return band_info\n\n\n# define function to extract all bands from letter\ndef extract_bands_from_letter(page_source):\n    # create empty list for all bands in each letter\n    entire_letter_bands = []\n    # set page_source equal to the driver's page_source\n    page_source = driver.page_source\n    # call extract_band_info function on each page and add on to entire_letter_bands\n    entire_letter_bands.extend(extract_band_info(page_source=page_source))\n\n    # instantiate logic\n    while True:\n        try:\n            # define the next button \n            next_button = driver.find_element(By.XPATH, '//*[@id=\"bandListAlpha_next\"]')\n            # if on last page (no next_button), break\n            if 'next paginate_button paginate_button_disabled' in next_button.get_attribute('class'):\n                break \n            # otherwise, click to next page\n            else: \n                next_button.click()\n                # give content a second to load\n                time.sleep(2)\n                # define page_source as the driver's page_source\n                page_source = driver.page_source\n                # recall extract_band_info function as long as still on same letter\n                entire_letter_bands.extend(extract_band_info(page_source=page_source))\n                # add logic to print exception and break\n        except Exception as e:\n            print(f\"Error: {e}\")\n            break\n        # return all bands from the letter\n    return entire_letter_bands\n\n\n# define function to get every band on metallum\ndef extract_all_letters(base_url):\n    # define base url\n    base_url = base_url\n    # define letter list\n    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    # define character list (NBR would have been evaluated as 'N', 'B', and 'R')\n    characters = ['NBR', '~']\n    # set letters_finished (loop-breaking arg) to False\n    letters_finished = False\n    # set characters_finished (loop-breaking arg) to False\n    characters_finished = False\n    # create empty list for all bands\n    all_bands = []\n    \n    # loop through list of letters\n    for letter in letters:\n        # unless already looped through, then break\n        if letters_finished:\n            break\n        try:\n            # set letter_page as base_url plus the letter\n            letter_page = f\"{base_url}{letter}\"\n            # print the url to monitor progress\n            print(f\"Accessing URL: {letter_page}\")\n            # get driver to access the letter page\n            driver.get(letter_page)\n            # give it 10 if it needs it\n            driver.implicitly_wait(10)\n            # extract the bands in each letter using previous function\n            bands_in_letter = extract_bands_from_letter(driver.page_source)\n            # add entries to all_bands\n            all_bands.extend(bands_in_letter)\n            # if letter is Z (end reached), set loop to break\n            if letter == 'Z':\n                letters_finished = True\n                # print any exceptions\n        except Exception as e:\n            print(f\"Error accessing {letter_page}: {e}\")\n            continue\n        \n    # character function is identical, but loops over the characters instead\n    for character in characters:\n        if characters_finished:\n            break\n        try:\n            # set character_page as base_url plus the character\n            character_page = f\"{base_url}{character}\"\n            # print the url to monitor progress\n            print(f\"Accessing URL: {character_page}\")\n            # get driver to access the character page\n            driver.get(character_page)\n            # give it 10 if it needs it\n            driver.implicitly_wait(10)\n            # extract the bands in each character using previous function\n            bands_in_character = extract_bands_from_letter(driver.page_source)\n            # add entries to all_bands\n            all_bands.extend(bands_in_character)\n            # if character is ~ (end reached), set loop to break \n            if character == '~':\n                characters_finished = True\n                # print any exceptions\n        except Exception as e:\n            print(f\"Error accessing {character_page}: {e}\")\n            continue    \n        \n        # return all the bands from all the letters and characters\n    return all_bands \n\n\n# using extract_all_letters, save all bands to all_bands\nall_bands = extract_all_letters(base_url=base_url)\n\nAccessing URL: https://www.metal-archives.com/lists/A\nAccessing URL: https://www.metal-archives.com/lists/B\nAccessing URL: https://www.metal-archives.com/lists/C\nAccessing URL: https://www.metal-archives.com/lists/D\nAccessing URL: https://www.metal-archives.com/lists/E\nAccessing URL: https://www.metal-archives.com/lists/F\nAccessing URL: https://www.metal-archives.com/lists/G\nAccessing URL: https://www.metal-archives.com/lists/H\nAccessing URL: https://www.metal-archives.com/lists/I\nAccessing URL: https://www.metal-archives.com/lists/J\nAccessing URL: https://www.metal-archives.com/lists/K\nAccessing URL: https://www.metal-archives.com/lists/L\nAccessing URL: https://www.metal-archives.com/lists/M\nAccessing URL: https://www.metal-archives.com/lists/N\nAccessing URL: https://www.metal-archives.com/lists/O\nAccessing URL: https://www.metal-archives.com/lists/P\nAccessing URL: https://www.metal-archives.com/lists/Q\nAccessing URL: https://www.metal-archives.com/lists/R\nAccessing URL: https://www.metal-archives.com/lists/S\nAccessing URL: https://www.metal-archives.com/lists/T\nAccessing URL: https://www.metal-archives.com/lists/U\nAccessing URL: https://www.metal-archives.com/lists/V\nAccessing URL: https://www.metal-archives.com/lists/W\nAccessing URL: https://www.metal-archives.com/lists/X\nAccessing URL: https://www.metal-archives.com/lists/Y\nAccessing URL: https://www.metal-archives.com/lists/Z\nAccessing URL: https://www.metal-archives.com/lists/NBR\nAccessing URL: https://www.metal-archives.com/lists/~\n\n\n\n# quit driver\ndriver.quit()\n\n\n# convert all_bands to df\nmetallum_df = pd.DataFrame.from_dict(all_bands)\n# create band_id column from index\nmetallum_df['band_id'] = metallum_df.index\n# output df to csv\nmetallum_df.to_csv('data/metallum_bands.csv')\n\n\n# import sqlite and create database 'metallum_bands.db'\nimport sqlite3\nconn = sqlite3.connect('metallum_bands.db')\n# add metallum_df to database\nmetallum_df.to_sql('metal_archives_table', con=conn, if_exists='replace', index=False)\n\n180137\n\n\n\n# commit database changes and close sqlite connection\nconn.commit()\nconn.close()"
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "",
    "text": "The analyses in this report analyzed commonly co-occurring Metal subgenres in Metal bands using Market Basket Analysis techniques.\nBands were scraped from Encyclopaedia Metallum and assigned binary values to each basic subgenre from the website. Only bands that were classified as having multiple subgenres were examined here.\nDeath (52.7%), Thrash (30.7%), and Black Metal (27.6%) were the most common subgenres in multi-subgenre bands. More common subgenres tended to have a lower percentage of multi-subgenre bands (e.g., Black Metal: 37.0%; Heavy Metal: 39.9%), whereas less common subgenres tended to have a higher percentage of multi-subgenre bands (e.g., Experimental/Avant-garde Metal: 94.2%; Electronic/Industrial Metal: 90.6%). Among multi-subgenre bands, 89.9% of Grindcore bands were also categorized as Death Metal, 74.4% of Metalcore/Deathcore bands were also categorized as Death Metal, and 63.5% of Folk/Viking/Pagan Metal bands were also categorized as Black Metal. The subgenre pairings that occurred at least 1.5x more frequently than expected included Power and Heavy Metal, Gothic and Doom Metal, Folk/Viking/Pagan and Black Metal, Grindcore and Death Metal, and Speed and Thrash Metal.\nThese subgenres pairings represent commonly co-occurring Metal subgenres, and each pairing can be explained within the developmental context of Metal music. Further analyses can expand on those conducted here by examining more specific subgenres or including more contextual information regarding the band’s use of a given subgenre."
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#load-packages-and-data",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#load-packages-and-data",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Load Packages and Data",
    "text": "Load Packages and Data\nFollowing the loading of necessary packages and setting of options, the variable con was set to represent the connection to database metallum_bands.db. The data from this table was extracted into the tibble te_metal_bands_df.\n\n# load necessary packages\nlibrary(tidyverse) \nlibrary(RSQLite) \nlibrary(knitr) \nlibrary(ggplot2)  \nlibrary(arules)\n\n\n# set global option to 3 digits to keep outputs from overflowing\noptions(digits=3)\n\n\n# set con equal to SQLite database metallum_bands.db\ncon &lt;- dbConnect(RSQLite::SQLite(), 'metallum_bands.db')\n\n\n# extract the table dc_processed genres as a tibble and get basic info \nte_metal_bands_df &lt;- dbGetQuery(con, 'SELECT * FROM dc_processed_genres') %&gt;%\n    as_tibble()\nstr(te_metal_bands_df)\n\ntibble [182,105 × 22] (S3: tbl_df/tbl/data.frame)\n $ band_id              : int [1:182105] 0 1 2 3 4 5 6 7 8 9 ...\n $ band_name            : chr [1:182105] \"A // Solution\" \"A 12 Gauge Tragedy\" \"A Balance of Power\" \"A Band Named Jon\" ...\n $ country              : chr [1:182105] \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ genre                : chr [1:182105] \"Crust Punk/Thrash Metal\" \"Deathcore\" \"Melodic Death Metal/Metalcore\" \"Brutal Death Metal/Grindcore\" ...\n $ status               : chr [1:182105] \"Split-up\" \"Split-up\" \"Active\" \"Active\" ...\n $ black                : int [1:182105] 0 0 0 0 0 0 1 0 0 1 ...\n $ death                : int [1:182105] 0 1 1 1 1 0 0 1 1 0 ...\n $ doom                 : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ electronic_industrial: int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ experimental         : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ folk                 : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ gothic               : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ grindcore            : int [1:182105] 0 0 0 1 0 0 0 0 0 0 ...\n $ groove               : int [1:182105] 0 0 0 0 0 0 0 0 1 0 ...\n $ heavy                : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ metalcore_deathcore  : int [1:182105] 0 1 1 0 0 0 0 1 1 0 ...\n $ power                : int [1:182105] 0 0 0 0 0 1 0 0 0 0 ...\n $ progressive          : int [1:182105] 0 0 0 0 0 1 0 0 0 0 ...\n $ speed                : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ symphonic            : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ thrash               : int [1:182105] 1 0 0 0 1 0 0 0 0 0 ...\n $ total_subgenres      : int [1:182105] 1 2 2 2 2 2 1 2 3 1 ..."
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#initial-data-wrangling-and-transaction-setup",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#initial-data-wrangling-and-transaction-setup",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Initial Data Wrangling and Transaction Setup",
    "text": "Initial Data Wrangling and Transaction Setup\nColumns pertaining to subgenres were binary (0/1), denoting absence or presence of subgenre for each band, respectively. While the majority of bands belonged to one subgenre, a considerable amount had two or more. Following the removal of bands with fewer than two subgenres, nearly 70,000 multi-subgenre bands remained. The average number of subgenres for this subgroup was 2.15, indicating a smaller proportion of bands with three to six subgenres. The object trans_bands was used to convert the binary subgenre variables to logical (TRUE/FALSE) and store each band as a transaction with subgenres as items. The summary and plot depict the relative frequencies of each subgenre, with Death (36,395), Thrash (21,187), and Black Metal (19,060) as the most common subgenres.\n\n# shorten the longer names for easier plotting and outputs\nlong_subs &lt;- c(elect = 'electronic_industrial', \n               exp = 'experimental', \n               core = 'metalcore_deathcore', \n               prog = 'progressive')\nte_metal_bands_df &lt;- te_metal_bands_df %&gt;%\n    rename(all_of(long_subs))\nstr(te_metal_bands_df)\n\ntibble [182,105 × 22] (S3: tbl_df/tbl/data.frame)\n $ band_id        : int [1:182105] 0 1 2 3 4 5 6 7 8 9 ...\n $ band_name      : chr [1:182105] \"A // Solution\" \"A 12 Gauge Tragedy\" \"A Balance of Power\" \"A Band Named Jon\" ...\n $ country        : chr [1:182105] \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ genre          : chr [1:182105] \"Crust Punk/Thrash Metal\" \"Deathcore\" \"Melodic Death Metal/Metalcore\" \"Brutal Death Metal/Grindcore\" ...\n $ status         : chr [1:182105] \"Split-up\" \"Split-up\" \"Active\" \"Active\" ...\n $ black          : int [1:182105] 0 0 0 0 0 0 1 0 0 1 ...\n $ death          : int [1:182105] 0 1 1 1 1 0 0 1 1 0 ...\n $ doom           : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ elect          : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ exp            : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ folk           : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ gothic         : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ grindcore      : int [1:182105] 0 0 0 1 0 0 0 0 0 0 ...\n $ groove         : int [1:182105] 0 0 0 0 0 0 0 0 1 0 ...\n $ heavy          : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ core           : int [1:182105] 0 1 1 0 0 0 0 1 1 0 ...\n $ power          : int [1:182105] 0 0 0 0 0 1 0 0 0 0 ...\n $ prog           : int [1:182105] 0 0 0 0 0 1 0 0 0 0 ...\n $ speed          : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ symphonic      : int [1:182105] 0 0 0 0 0 0 0 0 0 0 ...\n $ thrash         : int [1:182105] 1 0 0 0 1 0 0 0 0 0 ...\n $ total_subgenres: int [1:182105] 1 2 2 2 2 2 1 2 3 1 ...\n\n\n\n# get distribution for total_subgenres\nggplot(te_metal_bands_df, aes(x=total_subgenres)) +\n    geom_histogram(binwidth=1) +\n    labs(title='Number of Bands with Number of Subgenres',\n         x='Number of Subgenres', \n         y='Number of Bands')\n\n\n\n\n\n\n\n\n\n# remove bands with 0 or 1 subgenre\nref_te_metal_bands_df &lt;- te_metal_bands_df %&gt;%\n    filter(total_subgenres &gt; 1)\nstr(ref_te_metal_bands_df)\n\ntibble [69,067 × 22] (S3: tbl_df/tbl/data.frame)\n $ band_id        : int [1:69067] 1 2 3 4 5 7 8 10 15 17 ...\n $ band_name      : chr [1:69067] \"A 12 Gauge Tragedy\" \"A Balance of Power\" \"A Band Named Jon\" \"A Band of Orcs\" ...\n $ country        : chr [1:69067] \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ genre          : chr [1:69067] \"Deathcore\" \"Melodic Death Metal/Metalcore\" \"Brutal Death Metal/Grindcore\" \"Death/Thrash Metal\" ...\n $ status         : chr [1:69067] \"Split-up\" \"Active\" \"Active\" \"Active\" ...\n $ black          : int [1:69067] 0 0 0 0 0 0 0 0 0 0 ...\n $ death          : int [1:69067] 1 1 1 1 0 1 1 0 1 1 ...\n $ doom           : int [1:69067] 0 0 0 0 0 0 0 1 0 0 ...\n $ elect          : int [1:69067] 0 0 0 0 0 0 0 0 0 0 ...\n $ exp            : int [1:69067] 0 0 0 0 0 0 0 0 0 0 ...\n $ folk           : int [1:69067] 0 0 0 0 0 0 0 0 0 0 ...\n $ gothic         : int [1:69067] 0 0 0 0 0 0 0 0 0 0 ...\n $ grindcore      : int [1:69067] 0 0 1 0 0 0 0 0 0 0 ...\n $ groove         : int [1:69067] 0 0 0 0 0 0 1 0 0 1 ...\n $ heavy          : int [1:69067] 0 0 0 0 0 0 0 1 0 0 ...\n $ core           : int [1:69067] 1 1 0 0 0 1 1 0 1 0 ...\n $ power          : int [1:69067] 0 0 0 0 1 0 0 0 0 0 ...\n $ prog           : int [1:69067] 0 0 0 0 1 0 0 0 0 0 ...\n $ speed          : int [1:69067] 0 0 0 0 0 0 0 0 0 0 ...\n $ symphonic      : int [1:69067] 0 0 0 0 0 0 0 0 0 0 ...\n $ thrash         : int [1:69067] 0 0 0 1 0 0 0 0 0 0 ...\n $ total_subgenres: int [1:69067] 2 2 2 2 2 2 3 2 2 2 ...\n\n\n\n# get mean number of subgenres among those with multiple\nsubgenres_per_multi_bands &lt;- mean(ref_te_metal_bands_df$total_subgenres)\nsubgenres_per_multi_bands\n\n[1] 2.15\n\n\n\n# setup transactions object from arules for subgenre columns\ntrans_bands &lt;- ref_te_metal_bands_df %&gt;%\n    select(black:thrash) %&gt;% \n    # convert from binary to logical\n    mutate_all(as.logical) %&gt;% \n    transactions()\n\n\n# get basic summary\nsummary(trans_bands)\n\ntransactions as itemMatrix in sparse format with\n 69067 rows (elements/itemsets/transactions) and\n 16 columns (items) and a density of 0.134 \n\nmost frequent items:\n  death  thrash   black   heavy    doom (Other) \n  36395   21187   19060   10750   10205   50910 \n\nelement (itemset/transaction) length distribution:\nsizes\n    2     3     4     5     6 \n59353  9092   586    35     1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    2.00    2.00    2.15    2.00    6.00 \n\nincludes extended item information - examples:\n  labels variables levels\n1  black     black   TRUE\n2  death     death   TRUE\n3   doom      doom   TRUE\n\nincludes extended transaction information - examples:\n  transactionID\n1             1\n2             2\n3             3\n\n\n\n# show plot of each subgenre's relative frequency\nitemFrequencyPlot(trans_bands, topN=20)\n\n\n\n\nMetal Subgenres (left to right): Death, Thrash, Black, Heavy, Doom, Progressive, Groove, Power, Metalcore/Deathcore, Grindcore, Symphonic, Gothic, Speed, Folk/Viking/Pagan, Experimental/Avant-garde, Electronic/Industrial."
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#support",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#support",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Support",
    "text": "Support\nSupport represents the proportion of the records in a set of items containing an item or combination of items, as depicted in the equation below:\n\\[\nSupport = \\frac{Count(A)}{Count(All)}\n\\]\nThe threshold for support was set to 0.015, which is the proportion roughly equal to 1,000 bands in the multi-subgenre subset. The table for single subgenres reiterates the frequency plot from the previous section, with Death (52.7%), Thrash (30.7%), and Black Metal (27.6%) comprising a large proportion of the total bands. With a fairly liberal support threshold, 22 multi-subgenre combinations had 1,000 bands or more. Two immediate standouts included Death-Thrash and Blackened Death Metal, with both comprising roughly 14% of observations in the item set (14.1% and 14.0%, respectively).\n\n# create apriori rules object for occurrences with support &gt; 0.015\nsupport_rules &lt;- trans_bands %&gt;%\n    apriori(parameter=list(target='frequent', support=0.015))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n         NA    0.1    1 none FALSE            TRUE       5   0.015      1\n maxlen            target  ext\n     10 frequent itemsets TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 1036 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[16 item(s), 69067 transaction(s)] done [0.00s].\nsorting and recoding items ... [16 item(s)] done [0.00s].\ncreating transaction tree ... done [0.01s].\nchecking subsets of size 1 2 3 done [0.00s].\nsorting transactions ... done [0.01s].\nwriting ... [38 set(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\n# create subset for single subgenres\nsingle_item_support &lt;- subset(support_rules, size(items) == 1)\n\n\n# create subset for multiple subgenres\nmulti_item_support &lt;- subset(support_rules, size(items) &gt; 1)\n\n\n# sort and display the single subgenre support metrics\nsingle_item_support_sorted &lt;- sort(single_item_support, by='support') %&gt;%\n    head(n=20) %&gt;%\n    inspect()\n\n     items       support count\n[1]  {death}     0.5270  36395\n[2]  {thrash}    0.3068  21187\n[3]  {black}     0.2760  19060\n[4]  {heavy}     0.1556  10750\n[5]  {doom}      0.1478  10205\n[6]  {prog}      0.1123   7759\n[7]  {groove}    0.1032   7127\n[8]  {power}     0.1019   7035\n[9]  {core}      0.0975   6735\n[10] {grindcore} 0.0922   6366\n[11] {symphonic} 0.0550   3797\n[12] {gothic}    0.0432   2983\n[13] {speed}     0.0419   2892\n[14] {folk}      0.0397   2745\n[15] {exp}       0.0287   1979\n[16] {elect}     0.0216   1492\n\n\n\n# sort and display the multiple subgenre support metrics\nmulti_item_support_sorted &lt;- sort(multi_item_support, by='support') %&gt;%\n    head(n=25) %&gt;%\n    inspect()\n\n     items              support count\n[1]  {death, thrash}    0.1414  9766 \n[2]  {black, death}     0.1397  9650 \n[3]  {death, grindcore} 0.0829  5724 \n[4]  {death, core}      0.0725  5009 \n[5]  {death, doom}      0.0597  4125 \n[6]  {heavy, thrash}    0.0502  3466 \n[7]  {heavy, power}     0.0477  3294 \n[8]  {groove, thrash}   0.0443  3060 \n[9]  {black, thrash}    0.0433  2991 \n[10] {death, prog}      0.0402  2774 \n[11] {black, doom}      0.0368  2540 \n[12] {death, groove}    0.0284  1959 \n[13] {black, folk}      0.0252  1743 \n[14] {black, symphonic} 0.0218  1507 \n[15] {power, prog}      0.0197  1360 \n[16] {speed, thrash}    0.0196  1357 \n[17] {heavy, prog}      0.0188  1297 \n[18] {power, thrash}    0.0177  1221 \n[19] {prog, thrash}     0.0167  1154 \n[20] {doom, gothic}     0.0165  1139 \n[21] {doom, heavy}      0.0161  1114 \n[22] {groove, core}     0.0158  1094"
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#mixability",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#mixability",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Mixability",
    "text": "Mixability\nNext, I wanted to compare the proportion of bands featuring multiple subgenres across subgenres. To do this, I created a transactions object from the original data, which included all bands, regardless of how many subgenres each had. Then I extracted the support metrics from all bands and the bands with multiple subgenres, and joined these two dataframes into a single dataframe (comparison_support_df). I computed a new variable, mixability, as the proportion of each subgenre’s observations in which the subgenre was one of two or more:\n\\[\nMixability_{Subgenre} = P(2+ Subgenres) = \\frac{Count(Subgenre|2+ Subgenres)}{Count(Subgenre)}\n\\]\n\n# create transactions object for all bands regardless of subgenre number\nall_trans_bands &lt;- te_metal_bands_df %&gt;%\n    select(black:thrash) %&gt;%\n    mutate_all(as.logical) %&gt;%\n    transactions()\n\n\n# create ruleset from transations for support\nall_subs_support &lt;- all_trans_bands %&gt;%\n    apriori(parameter=(list(target='frequent', support=0.001)))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n         NA    0.1    1 none FALSE            TRUE       5   0.001      1\n maxlen            target  ext\n     10 frequent itemsets TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 182 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[16 item(s), 182105 transaction(s)] done [0.01s].\nsorting and recoding items ... [16 item(s)] done [0.00s].\ncreating transaction tree ... done [0.02s].\nchecking subsets of size 1 2 3 done [0.00s].\nsorting transactions ... done [0.02s].\nwriting ... [89 set(s)] done [0.00s].\ncreating S4 object  ... done [0.01s].\n\n\n\n# extract all single subgenre support metrics \nsingle_all_subs_support &lt;- subset(all_subs_support, size(items) == 1)\n\n\n# sort the single subgenre support metrics\nsingle_all_subs_support_sorted &lt;- sort(single_all_subs_support, \n                                       by='support') %&gt;%\n    head(n=20) %&gt;%\n    inspect()\n\n     items       support count\n[1]  {death}     0.33693 61357\n[2]  {black}     0.28298 51532\n[3]  {thrash}    0.19252 35059\n[4]  {heavy}     0.14786 26926\n[5]  {doom}      0.11062 20144\n[6]  {prog}      0.06577 11977\n[7]  {power}     0.05293  9639\n[8]  {groove}    0.04758  8664\n[9]  {core}      0.04582  8344\n[10] {grindcore} 0.04349  7919\n[11] {symphonic} 0.02603  4741\n[12] {gothic}    0.02271  4135\n[13] {folk}      0.02026  3690\n[14] {speed}     0.01795  3268\n[15] {exp}       0.01154  2101\n[16] {elect}     0.00904  1646\n\n\n\n# convert single multi-subgenre support to dataframe\nsingle_support_df &lt;- as(single_item_support_sorted, 'data.frame')\n\n\n# convert all subgenre support to dataframe\nall_single_support_df &lt;- as(single_all_subs_support_sorted, 'data.frame')\n\n\n# add multi_ prefix to distinguish columns\nsingle_support_df &lt;- single_support_df %&gt;%\n    rename_with(.cols=support:count, ~ paste0('multi_', .x))\n\n\n# join tables on items (subgenre)\ncomparison_support_df &lt;- single_support_df %&gt;%\n    left_join(all_single_support_df, join_by(items))\n\n\n# create mixability variable and sort accordingly\ncomparison_support_df &lt;- comparison_support_df %&gt;%\n    mutate(mixability = multi_count/count) %&gt;%\n    arrange(desc(mixability))"
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#mixability-results",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#mixability-results",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Mixability Results",
    "text": "Mixability Results\n\n# output comparison support metrics \ncomparison_support_df\n\n         items multi_support multi_count support count mixability\n1        {exp}        0.0287        1979 0.01154  2101      0.942\n2      {elect}        0.0216        1492 0.00904  1646      0.906\n3      {speed}        0.0419        2892 0.01795  3268      0.885\n4     {groove}        0.1032        7127 0.04758  8664      0.823\n5       {core}        0.0975        6735 0.04582  8344      0.807\n6  {grindcore}        0.0922        6366 0.04349  7919      0.804\n7  {symphonic}        0.0550        3797 0.02603  4741      0.801\n8       {folk}        0.0397        2745 0.02026  3690      0.744\n9      {power}        0.1019        7035 0.05293  9639      0.730\n10    {gothic}        0.0432        2983 0.02271  4135      0.721\n11      {prog}        0.1123        7759 0.06577 11977      0.648\n12    {thrash}        0.3068       21187 0.19252 35059      0.604\n13     {death}        0.5270       36395 0.33693 61357      0.593\n14      {doom}        0.1478       10205 0.11062 20144      0.507\n15     {heavy}        0.1556       10750 0.14786 26926      0.399\n16     {black}        0.2760       19060 0.28298 51532      0.370\n\n\n\n# create scatter plot comparing number of bands to mixability\nggplot(comparison_support_df, aes(x=count, y=mixability, label=items)) + \n    geom_text(check_overlap=TRUE) +\n    labs(title='Mixability in Relation to Total Subgenre Band Count',\n         x='Number of Bands in Subgenre', \n         y='Mixability (Proportion in Multi-Subgenre)')\n\n\n\n\nMetal Subgenres (left to right): Electronic/Industrial, Experimental/Avant-garde, Speed, Folk/Viking/Pagan, Gothic, Symphonic, Grindcore, Metalcore/Deathcore, Groove, Power, Progressive, Doom, Heavy, Thrash, Black, Death.\n\n\n\n\nA few different findings emerge from the scatter plot above:\n\nThere appears to be an inverse relationship between number of bands in each subgenre and subgenre mixability\nLess common subgenres (&lt; 10,000 bands) had high mixability (all subgenres besides Progressive were above &gt; .70)\nBlack, Heavy, and–to a lesser extent–Doom Metal had low mixability (.37, .40, and .51, respectively)\nThrash and Death Metal had relatively high mixability (.60 and .59, respectively), despite being more common subgenres\n\nWhile being appropriately speculative, it is plausible that more common subgenres have a greater capacity to be “stand-alone” subgenres. Conversely, subgenres like Experimental/Avant-garde and Electronic/Industrial Metal imply influences outside of Metal, which may reflect a lesser capacity for each to be a “stand-alone” Metal subgenre. Thrash and Death Metal had high mixability relative to subgenres of comparable size, such as Heavy or Black Metal. This may suggest Thrash and Death Metal have robust influences, which are simultaneously flexible enough to incorporate other subgenres more regularly than Heavy or Black Metal. Further investigation into the hierarchical structure of Metal subgenres may be useful to characterize mixability, and whether some of these high-mixability subgenres are treated more like add-on influences than subgenres."
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#confidence",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#confidence",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Confidence",
    "text": "Confidence\nConfidence represents the proportion of observations in which a consequent item (B) will also be in the interaction, given an antecedent item (A) in the association. It answers the question, “What are the chances I will get item B, if I already have A?”:\n\\[\nConfidence = \\frac{P(B|A)}{P(A)}\n\\]\nKeeping the same threshold for support (0.015), I added a confidence threshold of 0.25 to the apriori object rules. The combination of these constraints yielded item associations that occurred at least 1.5% of the time, in which the consequent (second) item was present at least 25% of the time, given the antecedent (first) item in the sequence. Only multi-item associations were examined, as single-item confidence metrics do not add anything on top of support.\n\n# extract rules with support &gt; 0.015 and confidence &gt; 0.25\nrules &lt;- apriori(trans_bands, parameter=list(support=0.015, \n                                             confidence=0.25))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.25    0.1    1 none FALSE            TRUE       5   0.015      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 1036 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[16 item(s), 69067 transaction(s)] done [0.00s].\nsorting and recoding items ... [16 item(s)] done [0.00s].\ncreating transaction tree ... done [0.01s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [20 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\n# extract multi-item associations only\nmulti_item_confidence &lt;- subset(rules, size(lhs(rules)) &gt; 0)\n\n\n# sort the multi-item assocations by confidence \nmulti_item_confidence_sorted &lt;- sort(multi_item_confidence, \n                                     by='confidence')\nmulti_item_confidence_sorted %&gt;%\n    inspect()\n\n     lhs            rhs      support confidence coverage lift  count\n[1]  {grindcore} =&gt; {death}  0.0829  0.899      0.0922   1.706 5724 \n[2]  {core}      =&gt; {death}  0.0725  0.744      0.0975   1.411 5009 \n[3]  {folk}      =&gt; {black}  0.0252  0.635      0.0397   2.301 1743 \n[4]  {black}     =&gt; {death}  0.1397  0.506      0.2760   0.961 9650 \n[5]  {speed}     =&gt; {thrash} 0.0196  0.469      0.0419   1.530 1357 \n[6]  {power}     =&gt; {heavy}  0.0477  0.468      0.1019   3.008 3294 \n[7]  {thrash}    =&gt; {death}  0.1414  0.461      0.3068   0.875 9766 \n[8]  {groove}    =&gt; {thrash} 0.0443  0.429      0.1032   1.400 3060 \n[9]  {doom}      =&gt; {death}  0.0597  0.404      0.1478   0.767 4125 \n[10] {symphonic} =&gt; {black}  0.0218  0.397      0.0550   1.438 1507 \n[11] {gothic}    =&gt; {doom}   0.0165  0.382      0.0432   2.584 1139 \n[12] {prog}      =&gt; {death}  0.0402  0.358      0.1123   0.678 2774 \n[13] {heavy}     =&gt; {thrash} 0.0502  0.322      0.1556   1.051 3466 \n[14] {heavy}     =&gt; {power}  0.0477  0.306      0.1556   3.008 3294 \n[15] {groove}    =&gt; {death}  0.0284  0.275      0.1032   0.522 1959 \n[16] {death}     =&gt; {thrash} 0.1414  0.268      0.5270   0.875 9766 \n[17] {death}     =&gt; {black}  0.1397  0.265      0.5270   0.961 9650 \n\n\nSeventeen multi-subgenre associations exceeded the .25 confidence threshold, with the top four exceeding a threshold of .50. As an example for the top entry, roughly 90% of Grindcore bands with multiple subgenres had Death Metal as a subgenre. Additionally, Death Metal is a common additional subgenre for Metalcore/Deathcore bands (74.4%), Black Metal is a common additional subgenre for Folk/Viking/Pagan Metal bands (63.5%), and Death Metal is a common additional subgenre for Black Metal bands (50.6%)."
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#lift",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#lift",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Lift",
    "text": "Lift\nLift goes a step beyond confidence, yielding the probably of consequent (B) given antecedent (A), accounting for the likelihood of these items occurring together if there was no association between them:\n\\[\nLift = \\frac{p(B|A)}{p(A)*p(B)}\n\\]\nSince no relationship is assumed between a pair of items, lift values can be interpreted with the following framework:\n\nLift = 0-1: occur less often than expected\nLift = 1: occurs as often as expected\nLift &gt; 1: occurs more often than expected\n\nUsing the same support (0.015) and confidence constraints (0.25) as before, the lift metrics are shown below in descending order.\n\n# support previous rules by lift \nlift_rules &lt;- sort(multi_item_confidence, by='lift')\nlift_rules %&gt;%\n    inspect()\n\n     lhs            rhs      support confidence coverage lift  count\n[1]  {power}     =&gt; {heavy}  0.0477  0.468      0.1019   3.008 3294 \n[2]  {heavy}     =&gt; {power}  0.0477  0.306      0.1556   3.008 3294 \n[3]  {gothic}    =&gt; {doom}   0.0165  0.382      0.0432   2.584 1139 \n[4]  {folk}      =&gt; {black}  0.0252  0.635      0.0397   2.301 1743 \n[5]  {grindcore} =&gt; {death}  0.0829  0.899      0.0922   1.706 5724 \n[6]  {speed}     =&gt; {thrash} 0.0196  0.469      0.0419   1.530 1357 \n[7]  {symphonic} =&gt; {black}  0.0218  0.397      0.0550   1.438 1507 \n[8]  {core}      =&gt; {death}  0.0725  0.744      0.0975   1.411 5009 \n[9]  {groove}    =&gt; {thrash} 0.0443  0.429      0.1032   1.400 3060 \n[10] {heavy}     =&gt; {thrash} 0.0502  0.322      0.1556   1.051 3466 \n[11] {black}     =&gt; {death}  0.1397  0.506      0.2760   0.961 9650 \n[12] {death}     =&gt; {black}  0.1397  0.265      0.5270   0.961 9650 \n[13] {death}     =&gt; {thrash} 0.1414  0.268      0.5270   0.875 9766 \n[14] {thrash}    =&gt; {death}  0.1414  0.461      0.3068   0.875 9766 \n[15] {doom}      =&gt; {death}  0.0597  0.404      0.1478   0.767 4125 \n[16] {prog}      =&gt; {death}  0.0402  0.358      0.1123   0.678 2774 \n[17] {groove}    =&gt; {death}  0.0284  0.275      0.1032   0.522 1959 \n\n\nIgnoring bidirectional rules shown in reverse order, nine subgenre pairings occurred more often than expected, with five occurring more than 1.5 times as frequently as expected. These pairings are shown in the table below. Using the top pairing of Power and Heavy Metal as an example, just under five percent (4.8%) of the multi-subgenre pairings included these two, but this was three times as frequently as expected (3.01).\n\n# filter to include only lift &gt; 1.5\n# filter confidence to remove duplicate heavy =&gt; power\nlifted_sub_pairs &lt;- subset(lift_rules, lift &gt; 1.5 & confidence &gt; .35)\nlifted_sub_pairs %&gt;%\n    inspect()\n\n    lhs            rhs      support confidence coverage lift count\n[1] {power}     =&gt; {heavy}  0.0477  0.468      0.1019   3.01 3294 \n[2] {gothic}    =&gt; {doom}   0.0165  0.382      0.0432   2.58 1139 \n[3] {folk}      =&gt; {black}  0.0252  0.635      0.0397   2.30 1743 \n[4] {grindcore} =&gt; {death}  0.0829  0.899      0.0922   1.71 5724 \n[5] {speed}     =&gt; {thrash} 0.0196  0.469      0.0419   1.53 1357"
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#conclusion",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#conclusion",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Conclusion",
    "text": "Conclusion\nReturning to the initial question regarding which metal subgenres frequently co-occur together, the lift table above yields a good picture of subgenres than co-occur more often than would be expected. These subgenre pairings are listed below:\n\nPower and Heavy Metal\nGothic and Doom Metal\nFolk/Viking/Pagan and Black Metal\nGrindcore and Death Metal\nSpeed and Thrash Metal\n\nTo knowledgeable Metalheads like myself, these pairings are unsurprising and sensible from a subgenre hierarchy standpoint. For an excellent visual representation of metal subgenres that contains contextual information on influences and timelines of each, I recommend visiting mapofmetal.com (Galbraith & Grant, n.d.). Two of these subgenre pairings represent an “offshoot” of the older subgenre (Power from Heavy Metal and Thrash from Speed Metal), which suggests a clear overlap between each of the pairings. Grindcore and Death Metal evolved on a similar timeline with similar influences, and the existence of intermediate subgenres like Deathgrind suggest a clear link between the two.\nThe links between Gothic and Doom and Folk/Viking/Pagan and Black Metal are present as well, and each represents interesting path that potentially diverged from the larger developments in a subgenre. For Gothic and Doom Metal (i.e., Gothic Doom Metal), it may have served as a means for Gothic Metal bands of the late 1990’s to diverge from the contemporary usage of symphonic elements in the subgenre (Gothic Doom, n.d.). Instead, Gothic Doom Metal leaned into the more bleak and depressing lyrical and musical influences inherent to both Gothic and Doom Metal. The link between Folk/Viking/Pagan and Black Metal likely stems from the subgenres Viking and Pagan Metal, which were both derived from Black Metal (Hofmann, 2020). These subgenres supplanted the Satanic lyrical themes typical of Black Metal by incorporating Norse mythology and Paganism into lyrics, as well as focuses on nature and broader folklore (Hofmann, 2020; Von Helden & Scott, 2010)."
  },
  {
    "objectID": "posts/metal_subgenre_pairings/subgenre_pairings.html#next-steps",
    "href": "posts/metal_subgenre_pairings/subgenre_pairings.html#next-steps",
    "title": "Metal Subgenre Market Basket Analysis in R",
    "section": "Next Steps",
    "text": "Next Steps\nSince these analyses were intended to provide a general overview of commonly co-occurring Metal subgenres, a couple improvements immediately come to mind for future analyses. Firstly, it would be advantageous to go beyond the basic subgenres provided by Encyclopaedia Metallum to analyze more specific subgenres. This is especially the case for base subgenres like Death or Doom Metal, which have a considerable degree of heterogeneity between subgenres under the base subgenre. Secondly, it would be useful to discern whether a band listed under multiple subgenres evolved from one to another, or simultaneously embodied multiple subgenres. Although these instances were treated as equivalent here, it could be argued that a band taking simultaneous influence from multiple subgenres is a better example of a multi-subgenre band. While Encyclopaedia Metallum provides labels like “(early)” or “(late)” to distinguish impermanent subgenres for a band, a proper demarcation might require additional information from album reviews or analysis or musical elements.\nThe following code block saves the dataframe te_metal_bands_df to a .csv file for later usage.\n\nwrite_csv(te_metal_bands_df, 'data/sub_metal_bands.csv')"
  },
  {
    "objectID": "posts/plate_discipline/code/plate_discipline_dr.html",
    "href": "posts/plate_discipline/code/plate_discipline_dr.html",
    "title": "Plate Discipline Dimensionality Reduction",
    "section": "",
    "text": "Major League hitters must balance three competing drives to provide offensive value for their team:\n1. Taking advantage of good pitches to hit\n2. Not swinging at pitches out of the zone, which are more likely to produce an out or an unfavorable count\n3. Making the pitcher throw more pitches to better gauge their arsenal and create a more stressful plate appearance\nAccordingly, hitters attempt to find an optimal strategy to create the most value for their team, which is highly individualized based on the player’s skill set and offensive role within the team.\nHere, I analyzed aggregated swing decision data (e.g., zone swing %, out of zone swing %, meatball swing percent %, and overall swing %) from qualifying MLB hitters within each season from 2021 to 2024. As an exploratory, proof-of-concept analysis, I performed Principal Component Analysis on these swing decision features to collapse similar sources of variance. The resulting principal components included a first component that captured a hitter’s tendency to swing in general, while the second component distinguished a hitter’s tendency to swing at good pitches while limiting the tendency to swing at bad ones. This pattern was largely consistent between seasons, as the first component had an intraclass correlation of .933 and the second had an intraclass correlation of .873.\nThese findings–while preliminary–suggest there may be more variance in how frequently a hitter swings more generally than a hitter’s ability to swing at good pitches (e.g., pitches in the zone, or meatballs in the middle of the zone) at the expense of swinging at pitches out of the zone. This calls into question the use of metrics like chase rate, as these may be confounded by a hitter’s tendency to swing more generally, rather than being entirely reflective of a hitter’s ability to not chase pitches. Future swing decision analyses can build off these results by investigating these constructs with greater specificity, including more contextual data that might impact swing decisions."
  },
  {
    "objectID": "posts/plate_discipline/code/plate_discipline_dr.html#tldr",
    "href": "posts/plate_discipline/code/plate_discipline_dr.html#tldr",
    "title": "Plate Discipline Dimensionality Reduction",
    "section": "",
    "text": "Major League hitters must balance three competing drives to provide offensive value for their team:\n1. Taking advantage of good pitches to hit\n2. Not swinging at pitches out of the zone, which are more likely to produce an out or an unfavorable count\n3. Making the pitcher throw more pitches to better gauge their arsenal and create a more stressful plate appearance\nAccordingly, hitters attempt to find an optimal strategy to create the most value for their team, which is highly individualized based on the player’s skill set and offensive role within the team.\nHere, I analyzed aggregated swing decision data (e.g., zone swing %, out of zone swing %, meatball swing percent %, and overall swing %) from qualifying MLB hitters within each season from 2021 to 2024. As an exploratory, proof-of-concept analysis, I performed Principal Component Analysis on these swing decision features to collapse similar sources of variance. The resulting principal components included a first component that captured a hitter’s tendency to swing in general, while the second component distinguished a hitter’s tendency to swing at good pitches while limiting the tendency to swing at bad ones. This pattern was largely consistent between seasons, as the first component had an intraclass correlation of .933 and the second had an intraclass correlation of .873.\nThese findings–while preliminary–suggest there may be more variance in how frequently a hitter swings more generally than a hitter’s ability to swing at good pitches (e.g., pitches in the zone, or meatballs in the middle of the zone) at the expense of swinging at pitches out of the zone. This calls into question the use of metrics like chase rate, as these may be confounded by a hitter’s tendency to swing more generally, rather than being entirely reflective of a hitter’s ability to not chase pitches. Future swing decision analyses can build off these results by investigating these constructs with greater specificity, including more contextual data that might impact swing decisions."
  },
  {
    "objectID": "posts/plate_discipline/code/plate_discipline_dr.html#overview-and-motivation-for-analyses",
    "href": "posts/plate_discipline/code/plate_discipline_dr.html#overview-and-motivation-for-analyses",
    "title": "Plate Discipline Dimensionality Reduction",
    "section": "1: Overview and Motivation for Analyses",
    "text": "1: Overview and Motivation for Analyses\nPut simply, hitting in the Major Leagues is one of the most challenging tasks in all of sports. Hitters must decide whether or not to swing in a mere fraction of a second, while pitchers throw multiple pitches to try and deceive the hitter. This creates a constant dilemma wherein hitters try not to miss good pitches to hit (e.g., pitches in the zone, or meatballs that are in the middle of the zone), while trying not to swing at pitches out of the zone, which are less optimal for hitting. Additionally, hitters are praised for “working the count” and seeing more pitches, as pitch counts have increasingly become a proxy for how fatigued and vulnerable a pitcher is to giving up runs. Chase rate and other swing-rate percentages are commonly used as proxies for how “disciplined” a hitter is (i.e., how likely or unlikely they are to swing at bad pitches), but it is still challenging to discern whether a hitter is truly good at discerning good from bad pitches, or if they are just employing a strategy to swing at fewer pitches to work the count.\nHaving played baseball for years as both a hitter and pitcher, I am continually fascinated with how hitters strike a balance between these competing priorities. Some hitters are more selective earlier in the count to ensure they see a few pitches, even if this means they might miss a good pitch to hit. Other hitters employ a more aggressive approach, which decreases the likelihood they will miss a good pitch to hit, but can leave them more vulnerable to swinging at less ideal pitches to hit. Striking an ideal balance between these competing goals requires good pitch discernment (similar terms include pitch recognition), which enables hitters to better discern good pitches to hit from bad ones. My goal in this report was to create a data-driven separation between hitting strategy and pitch discernment, as many common metrics of pitch selection capture some combination of both constructs (e.g., chase rate). Thus, I chose Principal Component Analysis as a way to collapse and restructure variance from correlated features to accomplish this."
  },
  {
    "objectID": "posts/plate_discipline/code/plate_discipline_dr.html#data-preparation",
    "href": "posts/plate_discipline/code/plate_discipline_dr.html#data-preparation",
    "title": "Plate Discipline Dimensionality Reduction",
    "section": "2: Data Preparation",
    "text": "2: Data Preparation\nThe data for these analyses comes from Baseball Savant’s Custom Leaderboard functionality. I selected the same columns for each season of data: 2021 (Baseball Savant, n.d.-a), 2022 (Baseball Savant, n.d.-b), 2023 (Baseball Savant, n.d.-c), and 2024 (Baseball Savant, n.d.-d). Only hitters with a qualifying number of plate appearances (502 in a season) were included, as this ensures all hitters had a large sample size. I performed the same analyses for each season, as this reduces any potential cohort effects specific to any given season. The metrics I chose to be features in the Principal Component Analysis model included the percentage a hitter swung at pitchers in the zone (z_swing_percent), out of the zone (oz_swing_percent), meatballs (meatball_swing_percent), and the percentage a hitter swung at all pitches (swing_percent). Baseball Savant did not provide a definition for a meatball, but the term generally implies a very hittable pitch in the middle of the strike zone.\nWhile I acknowledge that using these features does not take into account other important factors (e.g., the count the hitter is in, how the hitter is generally pitched, etc.), I wanted to focus solely on the hitter’s general tendencies for the sake of this proof-of-concept analysis. I discuss additional information that would add more context to these results in section 11: Future Directions.\nHere are the data preparation steps I took in sequential order:\n\nRead in the data from .csv files\nCreate pitches per plate appearance (pitches_per_pa), since this metrics was not available\nCreate a decisions_df_{year} dataframe for the features of interest for each year\nSeparate out default, performance-related columns into performance_df\nCreate a disc_metrics_df_{year} dataframe for the discipline metrics of interest for each year\nCreated a correlation plot among the 2024 features to show potential associations between each\n\n\n# import libraries to be used in these analyses\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport pingouin\nimport plotly.express as px\nfrom IPython.display import Markdown, display\n\n\n# import yearly data as csvs \ndisc_df_24 = pd.read_csv('../data/plate_discipline_24.csv')\ndisc_df_23 = pd.read_csv('../data/plate_discipline_23.csv')\ndisc_df_22 = pd.read_csv('../data/plate_discipline_22.csv')\ndisc_df_21 = pd.read_csv('../data/plate_discipline_21.csv')\n\n# inspect columns\ndisc_df_24.columns\n\nIndex(['last_name, first_name', 'player_id', 'year', 'pa', 'k_percent',\n       'bb_percent', 'woba', 'xwoba', 'sweet_spot_percent',\n       'barrel_batted_rate', 'hard_hit_percent', 'avg_best_speed',\n       'avg_hyper_speed', 'z_swing_percent', 'oz_swing_percent',\n       'oz_contact_percent', 'out_zone_percent', 'meatball_swing_percent',\n       'meatball_percent', 'pitch_count', 'iz_contact_percent',\n       'in_zone_percent', 'edge_percent', 'whiff_percent', 'swing_percent'],\n      dtype='object')\n\n\n\n# create new variable (pitches_pa) as the avg. number of pitches in a plate appearance\ndisc_df_24['pitches_pa'] = disc_df_24['pitch_count']/disc_df_24['pa']\ndisc_df_23['pitches_pa'] = disc_df_23['pitch_count']/disc_df_23['pa']\ndisc_df_22['pitches_pa'] = disc_df_22['pitch_count']/disc_df_22['pa']\ndisc_df_21['pitches_pa'] = disc_df_21['pitch_count']/disc_df_21['pa']\n\n\n# create decisions_df_ for each year as features of interest\ndecisions_df_24 = disc_df_24[[\n    'z_swing_percent', 'oz_swing_percent', 'meatball_swing_percent', 'swing_percent'\n    ]].copy()\ndecisions_df_23 = disc_df_23[[\n    'z_swing_percent', 'oz_swing_percent', 'meatball_swing_percent', 'swing_percent'\n    ]].copy()\ndecisions_df_22 = disc_df_22[[\n    'z_swing_percent', 'oz_swing_percent', 'meatball_swing_percent', 'swing_percent'\n    ]].copy()\ndecisions_df_21 = disc_df_21[[\n    'z_swing_percent', 'oz_swing_percent', 'meatball_swing_percent', 'swing_percent'\n    ]].copy()\n\n\n# save performance metrics for exploratory analysis\nperformance_df_24 = disc_df_24[[\n    'woba', 'xwoba', 'sweet_spot_percent', 'barrel_batted_rate', 'hard_hit_percent'\n]].copy()\n\n\n# create disc_metrics_df_ for each year as outputs of interest\ndisc_metrics_df_24 = disc_df_24[[\n    'pitches_pa', 'k_percent', 'bb_percent'\n]].copy()\ndisc_metrics_df_23 = disc_df_23[[\n    'pitches_pa', 'k_percent', 'bb_percent'\n]].copy()\ndisc_metrics_df_22 = disc_df_22[[\n    'pitches_pa', 'k_percent', 'bb_percent'\n]].copy()\ndisc_metrics_df_21 = disc_df_21[[\n    'pitches_pa', 'k_percent', 'bb_percent'\n]].copy()\n\n\n2.1: Feature Investigation\n\n# show scatter_matrix to illuminate relationships between features\nfig = px.scatter_matrix(\n        decisions_df_24, \n        dimensions=[\n            'swing_percent', 'z_swing_percent', 'oz_swing_percent', 'meatball_swing_percent'\n        ], \n        labels={\n            'swing_percent': 'Overall Swing',\n            'z_swing_percent': 'Zone Swing', \n            'oz_swing_percent': 'Out of Zone Swing', \n            'meatball_swing_percent': 'Meatball Swing', \n        }\n        )\n# remove diagonal and upper half\nfig.update_traces(\n    showupperhalf=False, diagonal_visible=False\n)\n\n                                                \n\n\n\nThe swing decision metrics are clearly related, which should be expected, given how each feature represents a hitter’s tendency to swing in a certain context. The following section will discuss the significance of this, and why Principal Component Analysis is an appropriate technique to use in this scenario."
  },
  {
    "objectID": "posts/plate_discipline/code/plate_discipline_dr.html#pca",
    "href": "posts/plate_discipline/code/plate_discipline_dr.html#pca",
    "title": "Plate Discipline Dimensionality Reduction",
    "section": "3: PCA",
    "text": "3: PCA\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique that re-captures the variance of dataset features as new, non-correlated components (GeeksforGeeks, 2025). This enables PCA to capture large portions of a datset’s variance with fewer components, as a Principal Component (PC) can simultaneously represent the same variance from multiple features. This made PCA an appropriate technique for this data, since I knew the four features of interest were highly correlated and related to some tendency to swing. Thus, I predicted that the first PC would relate to a hitter’s tendency to swing in general, since that was the commonality among the four features. Once the variance from the general tendency to swing was accounted for–and thus removed from the model–I predicted the remaining variance captured by the second PC would relate to a hitter’s pitch discernment (i.e., tendency to swing at good pitches instead of bad ones), since that was largely what differentiated the four features of interest.\n\n3.1: PCA Function\nBecause I intended to perform PCA on the same features for each year, I created a function that could run PCA and save the outputs for further investigation. Since I would need to map the PCs back to the original dataframes, I created dictionaries that linked season arguments (e.g., 2024) back to the disc_metrics_df_ and disc_df_ dataframes corresponding to that season.\nHere are the steps of the function laid out sequentially:\n\nStandardize the data so each feature has a mean of 0 and standard deviation of 1\nFit the PCA model to the standardized data to create a PCA model with the same number of PCs as the number of features\nCreate a covariance matrix to calculate eigenvalues and the proportion of variance captured by each PC\nStore eigenvalues and captured variance in a table and figures\nCreate a loadings table to show how PCs relate to the original features\nCreate a biplot figure to show superimposed loading scores on top of standardized data (code adapted from Plotly’s PCA documentation (Plotly, n.d.))\nTransform each player’s standardized data to PC1 and PC2 scores and map PC1 and PC2 columns back to original dataframe\nDefine correlation plot to show how PCs relate to discipline metrics (e.g., pitcher per plate appearance, strikeout percentage, and walk percentage)\nOrganize each output for easy calls later\n\n\n# create dictionary to map results back to its disc_metrics_df\ndisc_metrics_dict = {\n    \"2021\": disc_metrics_df_21,\n    \"2022\": disc_metrics_df_22,\n    \"2023\": disc_metrics_df_23,\n    \"2024\": disc_metrics_df_24\n}\n\n# create dictionary to map results back to its disc_df\ndisc_dict = {\n    \"2021\": disc_df_21,\n    \"2022\": disc_df_22,\n    \"2023\": disc_df_23,\n    \"2024\": disc_df_24\n}\n\n\n# accept dataframe and year (e.g., 2024) as arguments, along with year and df mappings\ndef run_PCA(df, disc_metrics_dict, disc_dict, year):\n    # scale data to have mean of 0 and sd of 1 (standardized)\n    scaler = StandardScaler()\n    # create as dataframe with original columns\n    standardized_decisions_df = pd.DataFrame(\n        scaler.fit_transform(df), columns = df.columns\n        )\n\n    # initialize PCA object, with number of components as length of features\n    pca = PCA(\n        n_components=standardized_decisions_df.shape[1]\n    )\n\n    # run PCA\n    pca.fit(standardized_decisions_df)\n    # fit PCA model to scaled data for further examination\n    vars_pca = pd.DataFrame(\n        pca.transform(standardized_decisions_df), \n        columns=['PC1', 'PC2', 'PC3', 'PC4']\n        )\n\n    # create covariance matrix as product of matrix multiplication, divided by number of observations\n    cov_matrix = np.dot(\n        standardized_decisions_df.T, \n        standardized_decisions_df\n        )/standardized_decisions_df.shape[0]\n    \n    # create array of eigenvalues using product of cov_matrix with each component\n    eigenvalues = np.array(\n        [\n            np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector))\n            for eigenvector in pca.components_\n            ]\n    )\n\n    # calculate the proportion of total variance each component accounts for\n    prop_var = eigenvalues/np.sum(eigenvalues)\n\n    # create dataframe to display Eigenvalue and Explained Variance for each PC\n    PC_values = pd.DataFrame(\n        {\n        'Eigenvalue': eigenvalues,\n        'Explained Variance': prop_var, \n        }, \n        index = ['PC1', 'PC2', 'PC3', 'PC4']\n        )\n\n    # create figure to plot eigenvalues for each PC\n    eigenvalue_fig = px.line(\n        PC_values, \n        x=PC_values.index,\n        y=\"Eigenvalue\", \n        labels={\n            'index': 'Principal Component', \n            'Eigenvalue': 'Eigenvalue'\n            }\n        )\n    # add horizontal line at 1 for people who like this rule\n    eigenvalue_fig.add_hline(y=1, line_color=\"red\", line_dash=\"dash\")\n\n    # create figure to plot eigenvalues for each PC\n    variance_fig = px.line(\n        PC_values, \n        x=PC_values.index,\n        y=\"Explained Variance\",\n        labels={\n            'index': 'Principal Component', \n            'Explained Variance': 'Variance Explained (Proportion)'\n            }\n    )\n\n    # create dataframe to map loading scores of original features to PCs \n    loadings = pd.DataFrame(\n        pca.components_.T, \n        columns = ['PC1', 'PC2', 'PC3', 'PC4'], \n        index = standardized_decisions_df.columns\n    )\n    \n    # save feature loadings for biplot figure\n    biplot_loadings = pca.components_\n\n    # save features \n    features = standardized_decisions_df.columns\n\n    # Code for biplot figure adapted from Plotly (n.d.)\n    # create biplot to visualize data and feature loadings in same figure\n    # initialize figure as scatterplot using observations in feature space\n    biplot_fig = px.scatter(vars_pca, x='PC1', y='PC2')\n\n    # loop through feature loading scores \n    # draw arrows from origin to loading scores on PC1 and PC2\n    for i, feature in enumerate(features):\n        biplot_fig.add_annotation(\n            ax=0, ay=0, \n            axref=\"x\", ayref=\"y\",\n            x=biplot_loadings[0, i],\n            y=biplot_loadings[1, i],\n            showarrow=True,\n            arrowsize=2,\n            arrowhead=2,\n            xanchor=\"right\",\n            yanchor=\"top\"\n        )\n        biplot_fig.add_annotation(\n            x=biplot_loadings[0, i],\n            y=biplot_loadings[1, i],\n            ax=0, ay=0, \n            xanchor=\"center\",\n            yanchor=\"bottom\",\n            text=feature,\n            yshift=5\n        )\n        # soften the data points to make feature loadings clearer\n        biplot_fig.update_traces(opacity=0.3)\n\n    # transform scaled data to create PC1 and PC2 scores for each hitter\n    PC1 = pd.DataFrame(\n        pca.fit_transform(standardized_decisions_df)[:, 0], \n        columns=['PC1']\n        )\n    PC2 = pd.DataFrame(\n        pca.fit_transform(standardized_decisions_df)[:, 1], \n        columns=['PC2']\n        )\n        \n    # create df strings for multiple uses\n    # map back toperformance_df_24_ using year_suffix\n    disc_metrics_dict[year]['PC1'] = PC1['PC1']\n    disc_metrics_dict[year]['PC2'] = PC2['PC2']\n\n    disc_dict[year]['PC1'] = PC1['PC1']\n    disc_dict[year]['PC2'] = PC2['PC2']\n\n    # create correlation plot for PCs and discipline metrics\n    corr_plot = px.scatter_matrix(\n        disc_metrics_dict[year], \n        dimensions=[\n            'PC1', 'PC2', 'pitches_pa', 'k_percent', 'bb_percent'\n        ], \n        labels={\n            'PC1': 'PC1', \n            'PC2': 'PC2', \n            'pitches_pa': 'Pitches per PA', \n            'k_percent': 'Strikeout %', \n            'bb_percent': 'Walk %'\n        }\n        )\n    corr_plot.update_traces(\n        showupperhalf=False, diagonal_visible=False\n    )\n\n\n    # define results set as outputs to call after running\n    return {\n        'pc_values': PC_values,\n        'loadings': loadings,\n        'disc_metrics_df': disc_metrics_dict[year],\n        'disc_df': disc_dict[year],\n        'eigenvalue_fig': eigenvalue_fig, \n        'variance_fig': variance_fig, \n        'biplot_fig': biplot_fig,\n        'corr_plot': corr_plot\n    }"
  },
  {
    "objectID": "posts/plate_discipline/code/plate_discipline_dr.html#pca-interpretation",
    "href": "posts/plate_discipline/code/plate_discipline_dr.html#pca-interpretation",
    "title": "Plate Discipline Dimensionality Reduction",
    "section": "PCA Interpretation",
    "text": "PCA Interpretation\nTo reiterate the PCA results, PC1 and PC2 combined to explain 97.2% of the variance in swing decisions. PC1 appeared captured a hitter’s tendency to swing at any type of pitch, while PC2 captured the hitter’s tendency to swing at any pitch or pitches out of the zone, at the expense of swinging at meatballs and pitches in the zone. As expected, PC1 was negatively associated with pitches per plate appearance and walk percentage, whereas PC2 was negatively associated with walk percentage. PC1 scores were positively associated with the percentage of pitches a hitter saw out of the zone, suggesting these hitters may see fewer good pitches to hit. Finally, neither PC1 nor PC2 exhibited strong associations with performance metrics, suggesting PC1 and PC2 scores alone are not related to the hitter’s performance.\nSince I had data for each seasons and wanted to model the same data for each, I performed the same PCA analyses in the following sections. Since I already explained the results and interpretation for 2024, I provided an overall interpretation for all years combined after running all PCA models (see section 8: Inter-Year Interpretation). I also forewent the exploratory analyses with the types of pitches a hitter saw and the hitter’s performance for subsequent seasons, as I did not have formal predictions and did not see a reason to re-run these."
  },
  {
    "objectID": "posts/plate_discipline/code/plate_discipline_dr.html#yearly-deviations",
    "href": "posts/plate_discipline/code/plate_discipline_dr.html#yearly-deviations",
    "title": "Plate Discipline Dimensionality Reduction",
    "section": "9.1: Yearly Deviations",
    "text": "9.1: Yearly Deviations\nTo add additional context to the ICC analyses, I calculated the root-mean squared deviation (RMSD) for PC1 and PC2, between seasons for each hitter. This created a mean score for each hitter to show how much their PC1 and PC2 scores varied between seasons, on average. When aggregated across players, this yields a picture of how much a typical hitters’ PC scores varied between seasons. This required the following data preparation steps:\n\nFind the mean PC1 and PC2 score for each hitter\nSubtract each individual season score from the hitter’s average score and square it (squared deviation)\nCreate a separate dataframe for RMSD calculations\nGroup by each hitter and calculate the average deviation between their actual and average score for each season, then square root that average (root mean)\nMerge the RMSD scores back onto original dataframe and drop duplicates for each hitter\nCalculate the mean, standard deviation, minimum, and maximum RMSD scores for PC1 and PC2 across all hitters\n\n\n# calculate the mean of PC1 for each player (across multiple seasons)\nfull_icc_PCs['PC1_mean'] = full_icc_PCs.groupby(\n    ['player_id', 'last_name, first_name']\n    )['PC1'].transform('mean')\n# calculate the mean of PC2 for each player (across multiple seasons)\nfull_icc_PCs['PC2_mean'] = full_icc_PCs.groupby(\n    ['player_id', 'last_name, first_name']\n    )['PC2'].transform('mean')\n\n# calculate the squared difference of PC1 and PC2 for each player across multiple seasons\nfull_icc_PCs['PC1_sq_dev'] = (full_icc_PCs['PC1'] - full_icc_PCs['PC1_mean']) ** 2\nfull_icc_PCs['PC2_sq_dev'] = (full_icc_PCs['PC2'] - full_icc_PCs['PC2_mean']) ** 2\nfull_icc_PCs\n\n\n\n\n\n\n\n\n\nlast_name, first_name\nplayer_id\nyear\nPC1\nPC2\nPC1_mean\nPC2_mean\nPC1_sq_dev\nPC2_sq_dev\n\n\n\n\n0\nReynolds, Bryan\n668804\n2024\n1.623757\n0.774924\n1.384978\n1.082272\n0.057016\n0.094463\n\n\n5\nFrance, Ty\n664034\n2024\n1.288773\n0.846018\n1.534494\n0.175227\n0.060379\n0.449961\n\n\n12\nFreeman, Freddie\n518692\n2024\n0.985310\n0.968539\n1.644380\n1.482795\n0.434373\n0.264459\n\n\n16\nOhtani, Shohei\n660271\n2024\n0.062435\n0.472165\n0.501415\n0.524333\n0.192704\n0.002721\n\n\n17\nMcMahon, Ryan\n641857\n2024\n-0.715191\n-0.153818\n-0.218904\n0.475158\n0.246301\n0.395610\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n506\nAlonso, Pete\n624413\n2021\n1.150744\n0.291504\n-0.209395\n-0.225707\n1.849979\n0.267507\n\n\n507\nSuárez, Eugenio\n553993\n2021\n-1.720105\n-0.018988\n-1.162140\n0.364468\n0.311325\n0.147039\n\n\n515\nMcMahon, Ryan\n641857\n2021\n0.765867\n1.093554\n-0.218904\n0.475158\n0.969773\n0.382414\n\n\n517\nOhtani, Shohei\n660271\n2021\n0.368540\n0.933051\n0.501415\n0.524333\n0.017656\n0.167051\n\n\n524\nOlson, Matt\n621566\n2021\n0.317050\n1.126588\n0.697248\n0.759598\n0.144551\n0.134682\n\n\n\n\n144 rows × 9 columns\n\n\n\n\n\n# create a separate df for calculating rmsd\nrmsd_df = (\n    # take full_icc_PCs and group by each hitter\n    full_icc_PCs.groupby(['player_id', 'last_name, first_name']).agg(\n        # aggregate to calculate mean, then square root it to get back on unit variance\n        RMSD_PC1=(\n            'PC1_sq_dev', lambda x: np.sqrt(x.mean())\n        ), \n        RMSD_PC2=(\n            'PC2_sq_dev', lambda x: np.sqrt(x.mean())\n        )\n    )\n    .reset_index()\n)\nrmsd_df.head()\n\n\n\n\n\n\n\n\n\nplayer_id\nlast_name, first_name\nRMSD_PC1\nRMSD_PC2\n\n\n\n\n0\n457759\nTurner, Justin\n0.585516\n0.232090\n\n\n1\n467793\nSantana, Carlos\n0.486479\n0.368101\n\n\n2\n502671\nGoldschmidt, Paul\n0.651286\n0.247912\n\n\n3\n518692\nFreeman, Freddie\n0.444667\n0.330299\n\n\n4\n543760\nSemien, Marcus\n0.785762\n0.349607\n\n\n\n\n\n\n\n\n\n# merge rmsd_df onto full_icc_PCs to get RMSD stats\nfull_icc_PCs = full_icc_PCs.merge(\n    rmsd_df[['player_id', 'RMSD_PC1', 'RMSD_PC2']], \n    # merge on player_id, only including where rmsd player_id matches\n    on='player_id', how='inner')\nfull_icc_PCs\n\n\n\n\n\n\n\n\n\nlast_name, first_name\nplayer_id\nyear\nPC1\nPC2\nPC1_mean\nPC2_mean\nPC1_sq_dev\nPC2_sq_dev\nRMSD_PC1\nRMSD_PC2\n\n\n\n\n0\nReynolds, Bryan\n668804\n2024\n1.623757\n0.774924\n1.384978\n1.082272\n0.057016\n0.094463\n0.271042\n0.472458\n\n\n1\nFrance, Ty\n664034\n2024\n1.288773\n0.846018\n1.534494\n0.175227\n0.060379\n0.449961\n0.408214\n0.406030\n\n\n2\nFreeman, Freddie\n518692\n2024\n0.985310\n0.968539\n1.644380\n1.482795\n0.434373\n0.264459\n0.444667\n0.330299\n\n\n3\nOhtani, Shohei\n660271\n2024\n0.062435\n0.472165\n0.501415\n0.524333\n0.192704\n0.002721\n0.475758\n0.561976\n\n\n4\nMcMahon, Ryan\n641857\n2024\n-0.715191\n-0.153818\n-0.218904\n0.475158\n0.246301\n0.395610\n0.681437\n0.465847\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n139\nAlonso, Pete\n624413\n2021\n1.150744\n0.291504\n-0.209395\n-0.225707\n1.849979\n0.267507\n1.549477\n0.486794\n\n\n140\nSuárez, Eugenio\n553993\n2021\n-1.720105\n-0.018988\n-1.162140\n0.364468\n0.311325\n0.147039\n0.664683\n0.287379\n\n\n141\nMcMahon, Ryan\n641857\n2021\n0.765867\n1.093554\n-0.218904\n0.475158\n0.969773\n0.382414\n0.681437\n0.465847\n\n\n142\nOhtani, Shohei\n660271\n2021\n0.368540\n0.933051\n0.501415\n0.524333\n0.017656\n0.167051\n0.475758\n0.561976\n\n\n143\nOlson, Matt\n621566\n2021\n0.317050\n1.126588\n0.697248\n0.759598\n0.144551\n0.134682\n0.329349\n0.262597\n\n\n\n\n144 rows × 11 columns\n\n\n\n\n\n# drop multiple references to each hitter\nsummary_icc_PCs = full_icc_PCs.drop_duplicates(subset='player_id')\n\n\n# calculate mean, standard deviation, minimum, and maximum for PC1 and PC2 \nmean_rmsd_PC1 = summary_icc_PCs['RMSD_PC1'].mean()\nstd_rmsd_PC1 = summary_icc_PCs['RMSD_PC1'].std()\nmin_rmsd_PC1 = summary_icc_PCs['RMSD_PC1'].min()\nmax_rmsd_PC1 = summary_icc_PCs['RMSD_PC1'].max() \n\nmean_rmsd_PC2 = summary_icc_PCs['RMSD_PC2'].mean()\nstd_rmsd_PC2 = summary_icc_PCs['RMSD_PC2'].std()\nmin_rmsd_PC2 = summary_icc_PCs['RMSD_PC2'].min()\nmax_rmsd_PC2 = summary_icc_PCs['RMSD_PC2'].max() \n\n# create dictionary for results to weave into dataframe\nrmsd_results = {\n    'Mean': [mean_rmsd_PC1, mean_rmsd_PC2],\n    'Standard Deviation': [std_rmsd_PC1, std_rmsd_PC2],\n    'Min': [min_rmsd_PC1, min_rmsd_PC2],\n    'Max': [max_rmsd_PC1, max_rmsd_PC2]\n}\n\n# create dataframe for easy display of aggregate stats\nrmsd_results_df = pd.DataFrame(\n    data=rmsd_results, \n    index=['PC1', 'PC2']\n)\n\n# display as markdown table\ndisplay(Markdown(rmsd_results_df.round(3).to_markdown()))\n\n\n\n\n\nMean\nStandard Deviation\nMin\nMax\n\n\n\n\nPC1\n0.566\n0.293\n0.068\n1.549\n\n\nPC2\n0.359\n0.149\n0.047\n0.644\n\n\n\n\n\n\nBy looking at the aggregated RMSD scores, we can see that PC1 scores varied by roughly 0.57 units, on average, while PC2 scores varied by roughly 0.36 units, on average. This makes sense, as PC1 captured more variance in the PCA models than PC2, and should have more within-hitter variance. I included the plot below to add context and show the distribution of RMSD scores.\n\n# create scatterplot to display RMSD scores for each player\nfig = px.scatter(\n    summary_icc_PCs, \n    x='RMSD_PC1',\n    y='RMSD_PC2', \n    # user player info as hover data\n    hover_data=['last_name, first_name', 'player_id'],\n    labels={\n        'RMSD_PC1': 'PC1 RMSD', \n        'RMSD_PC2': 'PC2 RMSD', \n        'last_name, first_name': 'Player'\n        },\n    title='PC1 vs. PC2 RMSD by Player (with 2021-2024 Data)'\n)\n\n# update size and opacity of markers\nfig.update_traces(\n    marker=dict(size=8, opacity=0.7)\n)\n# add lines for means of PC1 and PC2\nfig.add_hline(y=0.359, line_color=\"red\", line_dash=\"dash\")\nfig.add_vline(x=0.566, line_color=\"red\", line_dash=\"dash\")\nfig.show()\n\n                                                \n\n\n\nWhile the intent of the plot was to display what the typical variance of PC1 and PC2 scores looked like, two hitters immediately stuck out: Pete Alonso and Nathaniel Lowe. Using the plot at the end of the ICC section (see 9: Run Intraclass Correlations Between Years), I noticed a similar PC1 pattern for Alonso and Lowe. Alonso started with high PC1 scores in 2021 (1.15) and 2022 (1.51), but these scores drastically decreased to -1.62 in 2023 and -1.88 in 2024. Lowe started with a low PC1 score in 2021 (-1.30), went to a high score in 2022 (1.70), then back to a low score in 2023 (-0.86) and a very low score in 2024 (-2.16). Given the drastic deviations for Alonso and Lowe, it would be interesting to see if these could be explained by differences in how these two were pitched between seasons, or if they made deliberate changes to their approach between 2021 and 2024."
  },
  {
    "objectID": "posts/metal_subgenre_pairings/sql_subgenre_cleaning.html",
    "href": "posts/metal_subgenre_pairings/sql_subgenre_cleaning.html",
    "title": "Cleaning Metal Archives Data Using SQL",
    "section": "",
    "text": "Metal bands often incorporate influences from multiple subgenres, or shift subgenres throughout their career. Accordingly, many bands in the Metal Archives have a genre field with multiple subgenres. Rather than forcing each band into a primary subgenre, I wanted each band to be characterized as having a presence or absence of each subgenre. The code in this document accomplished these steps using SQL code by connecting to the metal_archives_table from the metallum_bands.db via an RSQLite instance.\n\n# import library from R\nlibrary(\"DBI\")\nlibrary(RSQLite)\nlibrary(knitr)\n# set options to print more rows (up to 100)\nopts_chunk$set(echo=TRUE, sql.max.print=100)\n\n\n# save connection to metallum_bands.db as con\ncon &lt;- dbConnect(RSQLite::SQLite(), \"metallum_bands.db\")\n\n\nSELECT * \nFROM metal_archives_table;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nband_name\ncountry\ngenre\nstatus\nband_id\n\n\n\n\nA // Solution\nUnited States\nCrust Punk/Thrash Metal\nSplit-up\n0\n\n\nA 12 Gauge Tragedy\nUnited States\nDeathcore\nSplit-up\n1\n\n\nA Balance of Power\nUnited States\nMelodic Death Metal/Metalcore\nActive\n2\n\n\nA Band Named Jon\nUnited States\nBrutal Death Metal/Grindcore\nActive\n3\n\n\nA Band of Orcs\nUnited States\nDeath/Thrash Metal\nActive\n4\n\n\nA Baptism by Fire\nCanada\nProgressive Power Metal\nActive\n5\n\n\nA Bastard’s Breath\nUnited States\nRaw Black Metal\nActive\n6\n\n\nA Belt Above Black\nUnited States\nMelodic Death Metal/Metalcore\nActive\n7\n\n\nA Billion Limbs\nUnited Kingdom\nGroove Metal/Deathcore\nActive\n8\n\n\nA Binding Spirit\nGermany\nBlack Metal/Ambient\nActive\n9"
  },
  {
    "objectID": "posts/metal_subgenre_pairings/sql_subgenre_cleaning.html#database-connection",
    "href": "posts/metal_subgenre_pairings/sql_subgenre_cleaning.html#database-connection",
    "title": "Cleaning Metal Archives Data Using SQL",
    "section": "",
    "text": "Metal bands often incorporate influences from multiple subgenres, or shift subgenres throughout their career. Accordingly, many bands in the Metal Archives have a genre field with multiple subgenres. Rather than forcing each band into a primary subgenre, I wanted each band to be characterized as having a presence or absence of each subgenre. The code in this document accomplished these steps using SQL code by connecting to the metal_archives_table from the metallum_bands.db via an RSQLite instance.\n\n# import library from R\nlibrary(\"DBI\")\nlibrary(RSQLite)\nlibrary(knitr)\n# set options to print more rows (up to 100)\nopts_chunk$set(echo=TRUE, sql.max.print=100)\n\n\n# save connection to metallum_bands.db as con\ncon &lt;- dbConnect(RSQLite::SQLite(), \"metallum_bands.db\")\n\n\nSELECT * \nFROM metal_archives_table;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nband_name\ncountry\ngenre\nstatus\nband_id\n\n\n\n\nA // Solution\nUnited States\nCrust Punk/Thrash Metal\nSplit-up\n0\n\n\nA 12 Gauge Tragedy\nUnited States\nDeathcore\nSplit-up\n1\n\n\nA Balance of Power\nUnited States\nMelodic Death Metal/Metalcore\nActive\n2\n\n\nA Band Named Jon\nUnited States\nBrutal Death Metal/Grindcore\nActive\n3\n\n\nA Band of Orcs\nUnited States\nDeath/Thrash Metal\nActive\n4\n\n\nA Baptism by Fire\nCanada\nProgressive Power Metal\nActive\n5\n\n\nA Bastard’s Breath\nUnited States\nRaw Black Metal\nActive\n6\n\n\nA Belt Above Black\nUnited States\nMelodic Death Metal/Metalcore\nActive\n7\n\n\nA Billion Limbs\nUnited Kingdom\nGroove Metal/Deathcore\nActive\n8\n\n\nA Binding Spirit\nGermany\nBlack Metal/Ambient\nActive\n9"
  },
  {
    "objectID": "posts/metal_subgenre_pairings/sql_subgenre_cleaning.html#steps-for-subgenre-structure",
    "href": "posts/metal_subgenre_pairings/sql_subgenre_cleaning.html#steps-for-subgenre-structure",
    "title": "Cleaning Metal Archives Data Using SQL",
    "section": "Steps for Subgenre Structure",
    "text": "Steps for Subgenre Structure\n\n1. Create TEMP Table for Processing Subgenres\nTo start, I created a TEMP table, processed_genres, to process the genre field in the data.\n\nCREATE TEMP TABLE processed_genres AS\nSELECT \n    band_id, \n    band_name,\n    country,\n    genre, \n    status\nFROM metal_archives_table;\n\n\n\n2. Create a New Column for Each Basic Metal Archives Subgenre\nOnce processed_genres was created, I added an integer column for each base subgenre from the Metal Archives.\n\nALTER TABLE processed_genres ADD COLUMN black INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN death INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN doom INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN electronic_industrial INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN experimental INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN folk INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN gothic INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN grindcore INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN groove INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN heavy INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN metalcore_deathcore INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN power INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN progressive INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN speed INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN symphonic INTEGER DEFAULT 0;\n\n\nALTER TABLE processed_genres ADD COLUMN thrash INTEGER DEFAULT 0;\n\n\nSELECT * \nFROM processed_genres\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nband_id\nband_name\ncountry\ngenre\nstatus\nblack\ndeath\ndoom\nelectronic_industrial\nexperimental\nfolk\ngothic\ngrindcore\ngroove\nheavy\nmetalcore_deathcore\npower\nprogressive\nspeed\nsymphonic\nthrash\n\n\n\n\n0\nA // Solution\nUnited States\nCrust Punk/Thrash Metal\nSplit-up\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\nA 12 Gauge Tragedy\nUnited States\nDeathcore\nSplit-up\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\nA Balance of Power\nUnited States\nMelodic Death Metal/Metalcore\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\nA Band Named Jon\nUnited States\nBrutal Death Metal/Grindcore\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nA Band of Orcs\nUnited States\nDeath/Thrash Metal\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nA Baptism by Fire\nCanada\nProgressive Power Metal\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nA Bastard’s Breath\nUnited States\nRaw Black Metal\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\nA Belt Above Black\nUnited States\nMelodic Death Metal/Metalcore\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n8\nA Billion Limbs\nUnited Kingdom\nGroove Metal/Deathcore\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\nA Binding Spirit\nGermany\nBlack Metal/Ambient\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n3. Identify Bands with Appropriate Subgenres\nThe code block below parsed the genre field in processed_genres and added a 1 to the value of a given subgenre when the band’s genre field included that subgenre. While I attempting to align with the Metal Archives’ original classifications, I made a few exceptions when an unclassified subgenre alone could be classified under an existing subgenre, without any additional phrasing or another subgenre. Examples of this are shown below, and reflected by inclusions to the code block below.\n\nSouthern alone would get put with both Doom/Stoner/Sludge and Groove\nGoregrind alone would get put with Grindcore\nNeoclassical alone would get put with Symphonic\n\nConversely, while Post-Metal typically went under Doom/Stoner/Sludge, anything with Post- in that subgenre was accompanied by Doom, Stoner, or Sludge. This suggests that Post-Metal alone was not enough to get grouped under Doom/Stoner/Sludge, and was therefore not classified under that subgenre here (Post-Metal and other unclassified subgenres are shown in section 3.A. Identify Records Not Belonging to a Subgenre). Since there was a vast amount of data to work with, no further processing was conducted for bands of unclassified subgenres.\n\nUPDATE processed_genres SET\n    black = CASE WHEN genre LIKE '%Black%' THEN 1 ELSE 0 END,\n    death = CASE WHEN genre LIKE '%Death%' THEN 1 ELSE 0 END,\n    doom = CASE WHEN genre LIKE '%Doom%' \n                OR genre LIKE '%Stoner%' \n                OR genre LIKE '%Sludge%' \n                OR genre LIKE '%Southern%' THEN 1 ELSE 0 END,\n    electronic_industrial = CASE WHEN genre LIKE '%Electronic%' \n                                 OR genre LIKE '%Industrial%' THEN 1 ELSE 0 END,\n    experimental = CASE WHEN genre LIKE '%Experimental%' \n                        OR genre LIKE '%Avant-garde%' THEN 1 ELSE 0 END,\n    folk = CASE WHEN genre LIKE '%Folk%' \n                OR genre LIKE '%Viking%'\n                OR genre LIKE '%Pagan%' THEN 1 ELSE 0 END,\n    gothic = CASE WHEN genre LIKE '%Gothic%' THEN 1 ELSE 0 END,\n    grindcore = CASE WHEN genre LIKE '%Grindcore%' \n                     OR genre LIKE '%Goregrind%' THEN 1 ELSE 0 END,\n    groove = CASE WHEN genre LIKE '%Groove%' \n                  OR genre LIKE '%Southern%' THEN 1 ELSE 0 END,\n    heavy = CASE WHEN genre LIKE '%Heavy%' THEN 1 ELSE 0 END,\n    metalcore_deathcore = CASE WHEN genre LIKE '%Metalcore%' \n                               OR genre LIKE '%Deathcore%' THEN 1 ELSE 0 END,\n    power = CASE WHEN genre LIKE '%Power%' THEN 1 ELSE 0 END,\n    progressive = CASE WHEN genre LIKE '%Progressive%' THEN 1 ELSE 0 END,\n    speed = CASE WHEN genre LIKE '%Speed%' THEN 1 ELSE 0 END,\n    symphonic = CASE WHEN genre LIKE '%Symphonic%' \n                     OR genre LIKE '%Neoclassical%' THEN 1 ELSE 0 END,\n    thrash = CASE WHEN genre LIKE '%Thrash%' THEN 1 ELSE 0 END;\n\n\nSELECT * \nFROM processed_genres\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nband_id\nband_name\ncountry\ngenre\nstatus\nblack\ndeath\ndoom\nelectronic_industrial\nexperimental\nfolk\ngothic\ngrindcore\ngroove\nheavy\nmetalcore_deathcore\npower\nprogressive\nspeed\nsymphonic\nthrash\n\n\n\n\n0\nA // Solution\nUnited States\nCrust Punk/Thrash Metal\nSplit-up\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\nA 12 Gauge Tragedy\nUnited States\nDeathcore\nSplit-up\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n2\nA Balance of Power\nUnited States\nMelodic Death Metal/Metalcore\nActive\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n3\nA Band Named Jon\nUnited States\nBrutal Death Metal/Grindcore\nActive\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nA Band of Orcs\nUnited States\nDeath/Thrash Metal\nActive\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n5\nA Baptism by Fire\nCanada\nProgressive Power Metal\nActive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n6\nA Bastard’s Breath\nUnited States\nRaw Black Metal\nActive\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\nA Belt Above Black\nUnited States\nMelodic Death Metal/Metalcore\nActive\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n8\nA Billion Limbs\nUnited Kingdom\nGroove Metal/Deathcore\nActive\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n9\nA Binding Spirit\nGermany\nBlack Metal/Ambient\nActive\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n3.A. Identify Records Not Belonging to a Subgenre\n\nALTER TABLE processed_genres ADD COLUMN total_subgenres INTEGER DEFAULT 0;\n\n\nUPDATE processed_genres\nSET total_subgenres = \n    black + death + doom + electronic_industrial + experimental + folk +  gothic + \n    grindcore + groove + heavy + metalcore_deathcore + power + progressive + speed + \n    symphonic + thrash;\n\n\nSELECT COUNT(*) AS subgenreless_bands \nFROM processed_genres\nWHERE total_subgenres = 0;\n\n\n1 records\n\n\nsubgenreless_bands\n\n\n\n\n403\n\n\n\n\n\n\nSELECT \n    genre, \n    COUNT(*) AS number_of_bands\nFROM processed_genres\nGROUP BY genre\nHAVING total_subgenres = 0\nORDER BY number_of_bands DESC\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\ngenre\nnumber_of_bands\n\n\n\n\nPost-Metal\n118\n\n\nCrossover\n90\n\n\nVarious\n73\n\n\nPost-Metal/Rock\n27\n\n\nCrossover/Hardcore\n8\n\n\nAtmospheric Post-Metal\n7\n\n\nGrind ‘n’ Roll\n6\n\n\nDark Ambient\n6\n\n\nPost-Metal/Shoegaze\n3\n\n\nVarious, Shred\n2\n\n\n\n\n\n\n\n\n4. Save Table\nThe code blocks below save the TEMP table processed_genres as dc_processed_genres, with the dc_ prefix denoting the dummy coding-like structure of the subgenre columns. Dropping any pre-existing tables of this name ensures the new table is created, even if a table with the same name exists.\n\nDROP TABLE IF EXISTS dc_processed_genres;\n\n\nCREATE TABLE dc_processed_genres AS \nSELECT * \nFROM processed_genres;\n\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/metal_subgenre_pairings/tableau_dash.html",
    "href": "posts/metal_subgenre_pairings/tableau_dash.html",
    "title": "Metal Subgenre Pairing Tableau Dashboard",
    "section": "",
    "text": "Below is the Tableau dashboard summarizing findings from subgenre_pairings.qmd."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Plate Discipline Dimensionality Reduction\n\n\n\n\n\n\nPython\n\n\nDimensionality Reduction\n\n\nData Mining\n\n\n\nDid you actually recognize that pitch? Or were you just taking all the way?\n\n\n\n\n\nJul 1, 2025\n\n\nKevin Miller\n\n\n\n\n\n\n\n\n\n\n\n\nMetal Subgenre Pairing Tableau Dashboard\n\n\n\n\n\n\nTableau\n\n\nData Visualization\n\n\n\nIn case you needed more, here is a Tableau dashboard depicting Metal subgenres that mix well.\n\n\n\n\n\nJul 10, 2024\n\n\nKevin Miller\n\n\n\n\n\n\n\n\n\n\n\n\nMetal Subgenre Market Basket Analysis in R\n\n\n\n\n\n\nR\n\n\nMarket Basket Analysis\n\n\nData Mining\n\n\n\nSubgenre pairings are just like how people pair wines and cheeses. Now it’s with music you probably dislike.\n\n\n\n\n\nJul 8, 2024\n\n\nKevin Miller\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning Metal Archives Data Using SQL\n\n\n\n\n\n\nSQL\n\n\nData Cleaning\n\n\n\nSince we all know multi-subgenre Metal bands are cooler, I parsed the genre field so each band could represent multiple subgenres.\n\n\n\n\n\nJul 8, 2024\n\n\nKevin Miller\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping Encyclopaedia Metallum\n\n\n\n\n\n\nPython\n\n\nWeb Scraping\n\n\n\nA web scraper designed to gather entries on hundreds of thousands of Metal bands.\n\n\n\n\n\nJul 7, 2024\n\n\nKevin Miller\n\n\n\n\n\n\n\n\n\n\n\n\nChi-Square and Random Forest Regression with Data Science Jobs Dataset\n\n\n\n\n\n\nPython\n\n\nDescriptive Analysis\n\n\nRandom Forest\n\n\n\nDo entry-level employees stand a chance in data science? Stay tuned to find out.\n\n\n\n\n\nMay 7, 2024\n\n\nKevin B. Miller\n\n\n\n\n\n\nNo matching items"
  }
]